{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#import needed libraries \nfrom urllib.request import urlretrieve\nimport zipfile, os\nimport time, sys, copy\nimport pandas as pd\nimport scipy.sparse as sps\nimport numpy as np\nfrom collections import defaultdict\nimport math","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clone repositeries\n!git clone https://github.com/shalini1194/RKT\n!git clone https://github.com/lyf-1/PEBG.git\n    \n#copy repositories in working directory\n!cp -r ./RKT/* ./\n#!cp -r ./PEBG/assist09/* ./\n#!cp -r ../input/skillbuilder-data-2009-2010/2012-2013-data-with-predictions-4-final.csv ./","execution_count":2,"outputs":[{"output_type":"stream","text":"Cloning into 'RKT'...\nremote: Enumerating objects: 53, done.\u001b[K\nremote: Counting objects: 100% (53/53), done.\u001b[K\nremote: Compressing objects: 100% (42/42), done.\u001b[K\nremote: Total 53 (delta 9), reused 44 (delta 7), pack-reused 0\u001b[K\nUnpacking objects: 100% (53/53), done.\nCloning into 'PEBG'...\nremote: Enumerating objects: 3, done.\u001b[K\nremote: Counting objects: 100% (3/3), done.\u001b[K\nremote: Compressing objects: 100% (3/3), done.\u001b[K\nremote: Total 50 (delta 0), reused 1 (delta 0), pack-reused 47\u001b[K\nUnpacking objects: 100% (50/50), done.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import from github cloned repositories\nfrom RKT import utils\nimport RKT.model_rkt as model_rkt\n","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Passages needed to recompute pro_pro_skills\n#Initially I will use file with the ones already computed by RKT authors\n#In the future I will modify the code and try to compute better ones.\n\n\"\"\" \nimport os \nimport pandas as pd\nimport numpy as np\nfrom scipy import sparse\n\n\nclass DataProcess():\n    def __init__(self, data_folder='assist09', file_name='skill_builder_data_corrected_collapsed.csv', min_inter_num=3):\n        print(\"Process Dataset %s\" % data_folder)\n        self.min_inter_num = min_inter_num\n        self.data_folder = data_folder\n        self.file_name = file_name\n\n    def process_csv(self):\n        #pre-process original csv file for assist dataset\n\n        # read csv file\n        data_path = os.path.join(self.data_folder, self.file_name)\n        df = pd.read_csv(data_path, low_memory=False, encoding=\"ISO-8859-1\")\n        print('original records number %d' % len(df))\n\n        # delete empty skill_id\n        df = df.dropna(subset=['skill_id'])\n        df = df[~df['skill_id'].isin(['noskill'])]\n        print('After removing empty skill_id, records number %d' % len(df))\n\n        # delete scaffolding problems\n        df = df[df['original'].isin([1])]\n        print('After removing scaffolding problems, records number %d' % len(df))\n\n        #delete the users whose interaction number is less than min_inter_num\n        users = df.groupby(['user_id'], as_index=True)\n        delete_users = []\n        for u in users:\n            if len(u[1]) < self.min_inter_num:\n                delete_users.append(u[0])\n        print('deleted user number based min-inters %d' % len(delete_users))\n        df = df[~df['user_id'].isin(delete_users)]\n        print('After deleting some users, records number %d' % len(df))\n        # print('features: ', df['assistment_id'].unique(), df['answer_type'].unique())\n\n        df.to_csv(os.path.join(self.data_folder, '%s_processed.csv'%self.file_name))\n\n\n    def pro_skill_graph(self):\n        df = pd.read_csv(os.path.join(self.data_folder, '%s_processed.csv'%self.file_name),low_memory=False, encoding=\"ISO-8859-1\")\n        problems = df['problem_id'].unique()\n        pro_id_dict = dict(zip(problems, range(len(problems))))\n        print('problem number %d' % len(problems))\n\n        pro_type = df['problem_type'].unique()\n        pro_type_dict = dict(zip(pro_type, range(len(pro_type))))\n        print('problem type: ', pro_type_dict)\n\n        pro_feat = []\n        pro_skill_adj = []\n        skill_id_dict, skill_cnt = {}, 0\n        for pro_id in range(len(problems)):            \n            tmp_df = df[df['problem_id']==problems[pro_id]]\n            tmp_df_0 = tmp_df.iloc[0]\n\n            # pro_feature: [ms_of_response, answer_type, mean_correct_num]\n            ms = tmp_df['ms_first_response'].abs().mean()\n            p = tmp_df['correct'].mean()\n            pro_type_id = pro_type_dict[tmp_df_0['problem_type']] \n            tmp_pro_feat = [0.] * (len(pro_type_dict)+2)\n            tmp_pro_feat[0] = ms\n            tmp_pro_feat[pro_type_id+1] = 1.\n            tmp_pro_feat[-1] = p\n            pro_feat.append(tmp_pro_feat)\n\n            # build problem-skill bipartite\n            tmp_skills =[]\n            for tmp_df_0 in tmp_df:\n                tmp_skills.append(tmp.tmp_df_0)\n            for s in tmp_skills:\n                if s not in skill_id_dict:\n                    skill_id_dict[s] = skill_cnt\n                    skill_cnt += 1\n                pro_skill_adj.append([pro_id, skill_id_dict[s], 1])\n\n        pro_skill_adj = np.array(pro_skill_adj).astype(np.int32)\n        pro_feat = np.array(pro_feat).astype(np.float32)\n        pro_feat[:, 0] = (pro_feat[:, 0] - np.min(pro_feat[:, 0])) / (np.max(pro_feat[:, 0])-np.min(pro_feat[:, 0]))\n        pro_num = np.max(pro_skill_adj[:, 0]) + 1\n        skill_num = np.max(pro_skill_adj[:, 1]) + 1\n        print('problem number %d, skill number %d' % (pro_num, skill_num))\n\n        # save pro-skill-graph in sparse matrix form\n        pro_skill_sparse = sparse.coo_matrix((pro_skill_adj[:, 2].astype(np.float32), (pro_skill_adj[:, 0], pro_skill_adj[:, 1])), shape=(pro_num, skill_num))\n        sparse.save_npz(os.path.join(self.data_folder, 'pro_skill_sparse.npz'), pro_skill_sparse)\n\n        # take joint skill as a new skill\n        skills = df['skill_id'].unique()\n        for s in skills:\n            if '_' in s:\n                skill_id_dict[s] = skill_cnt\n                skill_cnt += 1 \n\n        # save pro-id-dict, skill-id-dict\n        self.save_dict(pro_id_dict, os.path.join(self.data_folder, 'pro_id_dict.txt'))\n        self.save_dict(skill_id_dict, os.path.join(self.data_folder, 'skill_id_dict.txt'))\n\n        # save pro_feat_arr\n        np.savez(os.path.join(self.data_folder, 'pro_feat.npz'), pro_feat=pro_feat)\n\n    def generate_user_sequence(self, seq_file):\n        # generate user interaction sequence\n        # and write to data.txt\n\n        df = pd.read_csv(os.path.join(self.data_folder, '%s_processed.csv'%self.file_name), low_memory=False, encoding=\"ISO-8859-1\")\n        ui_df = df.groupby(['user_id'], as_index=True)   \n        print('user number %d' % len(ui_df))\n\n        user_inters = []\n        cnt = 0\n        for ui in ui_df:\n            tmp_user, tmp_inter = ui[0], ui[1]\n            tmp_problems = list(tmp_inter['problem_id'])\n            tmp_skills = list(tmp_inter['skill_id'])\n            tmp_ans = list(tmp_inter['correct'])\n            tmp_end_time = list(tmp_inter['end_time'])\n            user_inters.append([[len(tmp_inter)], tmp_skills, tmp_problems, tmp_ans, tmp_end_time])\n        \n        write_file = os.path.join(self.data_folder, seq_file)\n        self.write_txt(write_file, user_inters)\n\n\n    def save_dict(self, dict_name, file_name):\n        f = open(file_name, 'w')\n        f.write(str(dict_name))\n        f.close\n\n\n    def write_txt(self, file, data):\n        with open(file, 'w') as f:\n            for dd in data:\n                for d in dd:\n                    f.write(str(d)+'\\n')\n\n\n    def read_user_sequence(self, filename, max_len=200, min_len=3, shuffle_flag=True):\n        with open(filename, 'r') as f:\n            lines = f.readlines()\n        with open(os.path.join(self.data_folder, 'skill_id_dict.txt'), 'r') as f:\n            skill_id_dict = eval(f.read()) \n        with open(os.path.join(self.data_folder, 'pro_id_dict.txt'), 'r') as f:\n            pro_id_dict = eval(f.read())\n        \n\n        y, skill, problem, real_len, timestamp = [], [], [], [], []\n        skill_num, pro_num = len(skill_id_dict), len(pro_id_dict)\n        print('skill num, pro num, ', skill_num, pro_num)\n\n        index = 0\n        while index < len(lines):\n            num = eval(lines[index])[0]\n            tmp_skills = eval(lines[index+1])[:max_len]\n            # tmp_skills = [skill_id_dict[ele]+1 for ele in tmp_skills]     # for assist09\n            tmp_skills = [ele+1 for ele in tmp_skills]                      # for assist12 \n            tmp_pro = eval(lines[index+2])[:max_len]\n            tmp_pro = [pro_id_dict[ele]+1 for ele in tmp_pro]\n            tmp_ans = eval(lines[index+3])[:max_len]\n            tmp_time = eval(lines[index+4])[:max_len]\n\n            if num>=min_len:\n                tmp_real_len = len(tmp_skills)\n                # Completion sequence\n                tmp_ans += [-1]*(max_len-tmp_real_len)\n                tmp_skills += [0]*(max_len-tmp_real_len)\n                tmp_pro += [0]*(max_len-tmp_real_len)\n                tmp_time += [-1]*(max_len-tmp_real_len)\n\n                y.append(tmp_ans)\n                skill.append(tmp_skills)\n                problem.append(tmp_pro)\n                real_len.append(tmp_real_len)\n                timestamp\n\n            index += 5\n        \n        y = np.array(y).astype(np.float32)\n        skill = np.array(skill).astype(np.int32)\n        problem = np.array(problem).astype(np.int32)\n        real_len = np.array(real_len).astype(np.int32)\n\n        print(skill.shape, problem.shape, y.shape, real_len.shape)      \n        print(np.max(y), np.min(y))\n        print(np.max(real_len), np.min(real_len))  \n        print(np.max(skill), np.min(skill))\n        print(np.max(problem), np.min(problem))\n\n        np.savez(os.path.join(self.data_folder, \"%s.npz\"%self.file_name), problem=problem, y=y, skill=skill, real_len=real_len, skill_num=skill_num, problem_num=pro_num)\n\n\n\ndata_folder = './'\nmin_inter_num = 3\nfile_name='2012-2013-data-with-predictions-4-final.csv'\nDP = DataProcess(data_folder, file_name, min_inter_num)\n\nDP.process_csv()\nDP.pro_skill_graph()\nDP.generate_user_sequence('data.txt')\nDP.read_user_sequence(os.path.join(data_folder, 'data.txt')) \"\"\"\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Code from RKT train with few changes for performance\n\nimport argparse\nimport psutil\nimport gc\nimport pandas as pd\nfrom random import shuffle\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom scipy import sparse\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.nn.utils.rnn import pad_sequence\nfrom collections import  defaultdict\nfrom sys import getsizeof\n\nfrom RKT.model_rkt import RKT\nfrom RKT.utils import *\n\nstart = torch.cuda.Event(enable_timing=True)\nend = torch.cuda.Event(enable_timing=True)\n\nprint(torch.cuda.is_available())\n\n\ndef compute_corr(prob_seq, next_seq, corr_dic):\n    corr= np.zeros((prob_seq.shape[0],prob_seq.shape[1], prob_seq.shape[1]))\n    for i in range(0,prob_seq.shape[0]):\n        for  j in range(0,next_seq.shape[1] ):\n            for k in range(j+1):\n                corr[i][j][k]=corr_dic[next_seq[i][j]][prob_seq[i][k]]\n    return corr\ndef get_data(df, file2, max_length, train_split=0.8, randomize=True):\n    \"\"\"Extract sequences from dataframe.\n    Arguments:\n        df (pandas Dataframe): output by prepare_data.py\n        max_length (int): maximum length of a sequence chunk\n        train_split (float): proportion of data to use for training\n    \"\"\"\n\n    pro_pro_sparse = sparse.load_npz('./pro_pro_sparse.npz')\n\n    pro_pro_coo = pro_pro_sparse.tocoo()\n    # print(pro_skill_csr)\n    pro_pro_dense = pro_pro_coo.toarray()\n\n\n    item_ids = [torch.tensor(u_df[\"item_id\"].values, dtype=torch.long)\n                for _, u_df in df.groupby(\"user_id\")]\n    # skill_ids = [torch.tensor(u_df[\"skill_id\"].values, dtype=torch.long)\n    #              for _, u_df in df.groupby(\"user_id\")]\n    labels = [torch.tensor(u_df[\"correct\"].values, dtype=torch.long)\n              for _, u_df in df.groupby(\"user_id\")]\n\n\n\n    item_inputs = [torch.cat((torch.zeros(1, dtype=torch.long), i + 1))[:-1] for i in item_ids]\n    # skill_inputs = [torch.cat((torch.zeros(1, dtype=torch.long), s + 1))[:-1] for s in skill_ids]\n    label_inputs = [torch.cat((torch.zeros(1, dtype=torch.long), l))[:-1] for l in labels]\n\n    def chunk(list):\n        if list[0] is None:\n            return list\n        list = [torch.split(elem, max_length) for elem in list]\n        return [elem for sublist in list for elem in sublist]\n\n    # Chunk sequences\n    lists = (item_inputs, label_inputs, item_ids, labels)\n    chunked_lists = [chunk(l) for l in lists]\n\n    data = list(zip(*chunked_lists))\n    if randomize:\n        shuffle(data)\n\n    # Train-test split across users\n    train_size = int(train_split * len(data))\n    train_data, val_data = data[:train_size], data[train_size:]\n    return pro_pro_dense, train_data, val_data\n\n\ndef prepare_batches(train_data, batch_size, randomize=True):\n    \"\"\"Prepare batches grouping padded sequences.\n    Arguments:\n        data (list of lists of torch Tensor): output by get_data\n        batch_size (int): number of sequences per batch\n    Output:\n        batches (list of lists of torch Tensor)\n    \"\"\"\n    process = psutil.Process(os.getpid())\n    print('PB entered: ', process.memory_info().rss)\n    # if randomize:\n    #     shuffle(train_data)\n    batches = []\n    train_y, train_skill, train_problem, timestamps, train_real_len = train_data[0], train_data[1], train_data[2], train_data[3], train_data[4]\n    print('train_y created: ', process.memory_info().rss)\n    gc.collect()\n    print('train_data deleted: ', process.memory_info().rss)\n    item_ids = [torch.LongTensor(i).cuda() for i in train_problem]\n    print('train_data deleted: ', process.memory_info().rss)\n    timestamp = [torch.LongTensor(timestamp).cuda() for timestamp in timestamps]\n    print('train_data deleted: ', process.memory_info().rss)\n    labels = [torch.LongTensor(i).cuda() for i in train_y]\n    print('train_data deleted: ', process.memory_info().rss)\n    item_inputs = [torch.cat((torch.zeros(1, dtype=torch.long).cuda(), i))[:-1] for i in item_ids]\n    print('train_data deleted: ', process.memory_info().rss)\n    # skill_inputs = [torch.cat((torch.zeros(1, dtype=torch.long), s))[:-1] for s in skill_ids]\n    label_inputs = [torch.cat((torch.zeros(1, dtype=torch.long).cuda(), l))[:-1] for l in labels]\n    print('train_data deleted: ', process.memory_info().rss)\n    data = list(zip(item_inputs, label_inputs, item_ids, timestamp, labels))\n    print(getsizeof(data))\n    print(len(data))\n    print(batch_size)\n    print(getsizeof(item_inputs))\n    print(getsizeof(label_inputs))\n    print(getsizeof(item_ids))\n    print('data created: ', process.memory_info().rss)\n    del train_y\n    del train_skill\n    del train_problem\n    del timestamps\n    del train_data\n    del train_real_len\n    del item_inputs\n    del label_inputs\n    del item_ids\n    del timestamp\n    del labels\n    gc.collect(0)\n    print(gc.garbage)\n    print('train_y deleted: ', process.memory_info().rss)\n    for k in range(0, len(data), batch_size):\n        print(k)\n        batch = data[k:k + batch_size]\n        seq_lists = list(zip(*batch))\n        \n        inputs_and_ids = [pad_sequence(seqs, batch_first=True, padding_value=0)\n                          for seqs in seq_lists[:-1]]\n        labels = pad_sequence(seq_lists[-1], batch_first=True, padding_value=-1)  # Pad labels with -1\n        batches.append([*inputs_and_ids, labels])\n    return batches\n\n\ndef train_test_split(data, split=0.8):\n    n_samples = data[0].shape[0]\n    split_point = int(n_samples*split)\n    train_data, test_data = [], []\n    for d in data:\n        train_data.append(d[:split_point])\n        test_data.append(d[split_point:])\n    del data\n    return train_data, test_data\n\n\ndef compute_auc(preds, labels):\n    preds = preds[labels >= 0].flatten()\n    labels = labels[labels >= 0].float()\n    if len(torch.unique(labels)) == 1:  # Only one class\n        auc = accuracy_score(labels, preds.round())\n        acc = auc\n    else:\n        auc = roc_auc_score(labels, preds)\n        acc = accuracy_score(labels, preds.round())\n    return auc, acc\n\n\ndef compute_loss(preds, labels, criterion):\n    preds = preds[labels >= 0].flatten()\n    labels = labels[labels >= 0].float()\n    return criterion(preds, labels)\ndef computeRePos(time_seq, time_span):\n    batch_size = time_seq.shape[0]\n    size = time_seq.shape[1]\n\n    time_matrix= (torch.abs(torch.unsqueeze(time_seq, axis=1).repeat(1,size,1).reshape((batch_size, size*size,1)) - \\\n                 torch.unsqueeze(time_seq,axis=-1).repeat(1, 1, size,).reshape((batch_size, size*size,1))))\n\n    # time_matrix[time_matrix>time_span] = time_span\n    time_matrix = time_matrix.reshape((batch_size,size,size))\n\n\n    return (time_matrix)\ndef get_corr_data(pro_num):\n    pro_pro_file = pd.read_csv(\"../input/ednet-dataset/ednet_corr.csv\",names=[\"pro1\", \"pro2\", \"corr\"], dtype={\"row\":int, \"col\":int, \"corr\":float})\n    pro1List = pro_pro_file.pro1.tolist()\n    pro2List = pro_pro_file.pro2.tolist()\n    corrList = pro_pro_file[\"corr\"].tolist()\n    print(pro_pro_file.memory_usage().sum() / 1024 / 1024)\n    pro_pro_sparse = sps.coo_matrix((corrList, (pro1List, pro2List)))\n    pro_pro_sparse = pro_pro_sparse.tocsr()\n    del (pro_pro_file, pro1List, pro2List, corrList)\n    return pro_pro_sparse\n\n\ndef train(train_data, val_data, pro_num, corr_data, timestamp, timespan,  model, optimizer, logger, saver, num_epochs, batch_size, grad_clip):\n    \"\"\"Train SAKT model.\n    Arguments:\n        train_data (list of tuples of torch Tensor)\n        val_data (list of tuples of torch Tensor)\n        model (torch Module)\n        optimizer (torch optimizer)\n        logger: wrapper for TensorboardX logger\n        saver: wrapper for torch saving\n        num_epochs (int): number of epochs to train for\n        batch_size (int)\n        grad_clip (float): max norm of the gradients\n    \"\"\"\n    process = psutil.Process(os.getpid())\n    print('entered train', process.memory_info().rss)\n    criterion = nn.BCEWithLogitsLoss()\n    step = 0\n    metrics = Metrics()\n    print('PB memory used: ', process.memory_info().rss)\n    \n    for epoch in range(num_epochs):\n        print(\"in epoch\"+str(epoch))\n        print(\"Prepare batches train\")\n        train_batches = prepare_batches(train_data, batch_size)\n        gc.collect()\n        print(\"Prepare batches val\")\n        val_batches = prepare_batches(val_data, batch_size)\n        gc.collect()\n        # Training\n        i=0\n        for item_inputs, label_inputs, item_ids, timestamp, labels in train_batches:\n            # rel = compute_corr(item_inputs, item_ids, corr_data)\n            print(i)\n            rel = corr_data[(item_ids-1).cpu().unsqueeze(1).repeat(1,item_ids.cup().shape[-1],1),(item_inputs.cup()-1).unsqueeze(-1).repeat(1,1,item_inputs.cpu().shape[-1])]\n            item_inputs = item_inputs.cuda()\n            time = computeRePos(timestamp, timespan)\n            # skill_inputs = skill_inputs.cuda()\n            label_inputs = label_inputs.cuda()\n            item_ids = item_ids.cuda()\n            # skill_ids = skill_ids.cuda()\n            preds, weights = model(item_inputs, label_inputs, item_ids, torch.Tensor(rel).cuda(), time.cuda())\n\n            loss = compute_loss(preds, labels.cuda(), criterion)\n            preds = torch.sigmoid(preds).detach().cpu()\n            train_auc, train_acc = compute_auc(preds, labels)\n            model.zero_grad()\n            loss.backward()\n            clip_grad_norm_(model.parameters(), grad_clip)\n            optimizer.step()\n            step += 1\n            metrics.store({'loss/train': loss.item()})\n            metrics.store({'auc/train': train_auc})\n\n            # Logging\n            if step == len(train_batches)-1:\n                torch.save(weights, 'weight_tensor_rel')\n            # print(step)\n            if step % 1000 == 0:\n                print(metrics.average())\n                print(step)\n\n                # weights = {\"weight/\" + name: param for name, param in model.named_parameters()}\n                # grads = {\"grad/\" + name: param.grad\n                #         for name, param in model.named_parameters() if param.grad is not None}\n                # logger.log_histograms(weights, step)\n                # logger.log_histograms(grads, step)\n            i+=1\n            if i%1000==0 :\n                gc.collect()\n            \n\n        # Validation\n\n        model.eval()\n        for item_inputs, label_inputs, item_ids, timestamp, labels in val_batches:\n            rel = corr_data[(item_ids - 1).unsqueeze(1).repeat(1, item_ids.shape[-1], 1), (item_inputs - 1).unsqueeze(-1).repeat(1,1, item_inputs.shape[-1])]\n            item_inputs = item_inputs.cuda()\n            # skill_inputs = skill_inputs.cuda()\n            time = computeRePos(timestamp, timespan)\n            label_inputs = label_inputs.cuda()\n            item_ids = item_ids.cuda()\n            # skill_ids = skill_ids.cuda()\n            with torch.no_grad():\n                preds,weights = model(item_inputs, label_inputs, item_ids, torch.Tensor(rel).cuda(), time.cuda())\n\n                preds = torch.sigmoid(preds).cpu()\n\n\n            val_auc, val_acc = compute_auc(preds, labels)\n            metrics.store({'auc/val': val_auc, 'acc/val': val_acc})\n            gc.collect()\n        model.train()\n\n        # Save model\n\n        average_metrics = metrics.average()\n        logger.log_scalars(average_metrics, step)\n        print(average_metrics)\n        stop = saver.save(average_metrics['auc/val'], model)\n        if stop:\n            break\n\n\nparser = argparse.ArgumentParser(description='Train RKT.')\nparser.add_argument('--dataset', type=str)\nparser.add_argument('--logdir', type=str, default='runs/rkt')\nparser.add_argument('--savedir', type=str, default='save/rkt')\nparser.add_argument('--max_length', type=int, default=200)\nparser.add_argument('--embed_size', type=int, default=200)\nparser.add_argument('--num_attn_layers', type=int, default=1)\nparser.add_argument('--num_heads', type=int, default=5)\nparser.add_argument('--encode_pos', action='store_true')\nparser.add_argument('--max_pos', type=int, default=10)\nparser.add_argument('--drop_prob', type=float, default=0.2)\nparser.add_argument('--batch_size', type=int, default=1024)\nparser.add_argument('--lr', type=float, default=1e-3)\nparser.add_argument('--grad_clip', type=float, default=10)\nparser.add_argument('--num_epochs', type=int, default=20)\nparser.add_argument('--timespan', default=100000, type=int)\nargs2 = {\"batch_size\": 50,\n         \"train_steps\": 1000,\n         \"logdir\": 'runs/rkt',\n         \"savedir\": 'save/rkt',\n         \"max_length\": 200,\n         \"embed_size\": 200,\n         \"num_attn_layers\": 1,\n         \"num_heads\": 5,\n         \"encode_pos\": True,\n         \"max_pos\": 10,\n         \"drop_prob\": 0.2,\n         \"batch_size\": 200,\n         \"lr\": 1e-3,\n         \"grad_clip\":10.0,\n         \"num_epochs\": 300,\n         \"timespan\": 100000}\n\nargs = parser.parse_args(args=[])\n\n# full_df = pd.read_csv('./', sep=\",\")\n# train_df = pd.read_csv('../../KT-GAT/data/ed_net2_train.csv', sep=\",\")\n# test_df = pd.read_csv('../../KT-GAT/data/ed_net2_test.csv', sep=\",\")\n# # train_data_file = '../KT-GAT/data/ed_net.csv'\n# corr_dic, train_data, val_data = get_data(train_df, '../../KT-GAT/RKT/ednet_corr', args.max_length)\n# print(len(train_data))\nprocess = psutil.Process(os.getpid())\ngc.enable()\ndata = np.load('../input/ednet-dataset/ednet.npz')\nprint('df created', process.memory_info().rss)\n\ny, skill, problem, timestamp, real_len = data['y'], data['skill'], data['problem'], data['time'] , data['real_len']\nskill_num, pro_num = data['skill_num'], data['problem_num']\nprint('problem number %d, skill number %d' % (pro_num, skill_num))\nprint('before data delete', process.memory_info().rss)\ndel data\ngc.collect()\nprint(gc.garbage)\nprint('after data delete', process.memory_info().rss)\ncorr_data = get_corr_data(pro_num)\nprint('coor_data created', process.memory_info().rss)\n# divide train test set\ntrain_data, test_data = train_test_split([y, skill, problem, timestamp, real_len])\nprint('y,skill,problem created', process.memory_info().rss)\ndel (y, skill, problem)\nprint(gc.garbage)\ngc.collect()\nprint('y, skill, problem deleted', process.memory_info().rss)\nnum_items = pro_num\n# num_items = int(full_df[\"item_id\"].max() + 1)\n# num_skills = int(full_df[\"skill_id\"].max() + 1)\n\nmodel = RKT(num_items, args.embed_size, args.num_attn_layers, args.num_heads,\n              args.encode_pos, args.max_pos, args.drop_prob).cuda()\noptimizer = Adam(model.parameters(), lr=args.lr)\n\n# Reduce batch size until it fits on GPU\nwhile True:\n    #try:\n        # Train\n    param_str = (f'{args.dataset},'\n                 f'batch_size={args.max_length},'\n                 f'max_length={args.max_length},'\n                 f'encode_pos={args.encode_pos},'\n                 f'max_pos={args.max_pos}')\n    logger = Logger(os.path.join(args.logdir, param_str))\n    saver = Saver(args.savedir, param_str)\n    print('before train', process.memory_info().rss)\n    train(train_data, test_data, pro_num, corr_data, timestamp, args.timespan, model, optimizer, logger, saver, args.num_epochs,\n          args.batch_size, args.grad_clip)\n    break\n    #except RuntimeError:\n     #   args.batch_size = args.batch_size // 2\n      #  print(RuntimeError)\n       # print(f'Batch does not fit on gpu, reducing size to {args.batch_size}')\n\nlogger.close()\n\nparam_str = (f'{args.dataset},'\n              f'batch_size={args.batch_size},'\n              f'max_length={args.max_length},'\n              f'encode_pos={args.encode_pos},'\n              f'max_pos={args.max_pos}')\nsaver = Saver(args.savedir, param_str)\nmodel = saver.load()\n# test_data, _ = get_data(test_df, args.max_length, train_split=1.0, randomize=False)\ntest_batches = prepare_batches(test_data, args.batch_size, randomize=False)\ncorr_data = get_corr_data(pro_num)\ntest_preds = np.empty(0)\n\n# Predict on test set\nprint(\"pre eval\")\nmodel.eval()\nprint(\"post eval\")\ncorrect = np.empty(0)\nfor item_inputs, label_inputs, item_ids, labels in test_batches:\n    print(\"in loop\")\n    rel = corr_data[\n        (item_ids - 1).unsqueeze(1).repeat(1, item_ids.shape[-1], 1), (item_inputs - 1).unsqueeze(-1).repeat(1, 1, item_inputs.shape[ -1])]\n    item_inputs = item_inputs.cuda()\n    # skill_inputs = skill_inputs.cuda()\n    label_inputs = label_inputs.cuda()\n    item_ids = item_ids.cuda()\n    # skill_ids = skill_ids.cuda()\n    with torch.no_grad():\n        preds = model(item_inputs, label_inputs, item_ids)\n        preds = torch.sigmoid(preds[labels >= 0]).flatten().cpu().numpy()\n        test_preds = np.concatenate([test_preds, preds])\n    labels = labels[labels>=0].float()\n    correct = np.concatenate([correct, labels])\n    print(correct)\n\n\nprint(correct.shape)\nprint(test_preds.shape)\nprint(\"auc_test = \", roc_auc_score(correct, test_preds))\nprint(\"acc_test = \", accuracy_score(correct, test_preds))","execution_count":4,"outputs":[{"output_type":"stream","text":"True\ndf created 353980416\nproblem number 12372, skill number 1901\nbefore data delete 3298570240\n[]\nafter data delete 3298570240\n617.0325241088867\ncoor_data created 3624140800\ny,skill,problem created 3624140800\n[]\ny, skill, problem deleted 3624140800\nbefore train 5173755904\nentered train 5173755904\nPB memory used:  5173755904\nin epoch0\nPrepare batches train\nPB entered:  5173755904\ntrain_y created:  5173755904\ntrain_data deleted:  5173755904\ntrain_data deleted:  5596889088\ntrain_data deleted:  6026342400\ntrain_data deleted:  6438612992\ntrain_data deleted:  7170994176\ntrain_data deleted:  7867719680\n5150408\n588157\n1024\n4826320\n4826320\n4826320\ndata created:  7929626624\n[]\ntrain_y deleted:  7929626624\n0\n1024\n2048\n3072\n4096\n5120\n6144\n7168\n8192\n9216\n10240\n11264\n12288\n13312\n14336\n15360\n16384\n17408\n18432\n19456\n20480\n21504\n22528\n23552\n24576\n25600\n26624\n27648\n28672\n29696\n30720\n31744\n32768\n33792\n34816\n35840\n36864\n37888\n38912\n39936\n40960\n41984\n43008\n44032\n45056\n46080\n47104\n48128\n49152\n50176\n51200\n52224\n53248\n54272\n55296\n56320\n57344\n58368\n59392\n60416\n61440\n62464\n63488\n64512\n65536\n66560\n67584\n68608\n69632\n70656\n71680\n72704\n73728\n74752\n75776\n76800\n77824\n78848\n79872\n80896\n81920\n82944\n83968\n84992\n86016\n87040\n88064\n89088\n90112\n91136\n92160\n93184\n94208\n95232\n96256\n97280\n98304\n99328\n100352\n101376\n102400\n103424\n104448\n105472\n106496\n107520\n108544\n109568\n110592\n111616\n112640\n113664\n114688\n115712\n116736\n117760\n118784\n119808\n120832\n121856\n122880\n123904\n124928\n125952\n126976\n128000\n129024\n130048\n131072\n132096\n133120\n134144\n135168\n136192\n137216\n138240\n139264\n140288\n141312\n142336\n143360\n144384\n145408\n146432\n147456\n148480\n149504\n150528\n151552\n152576\n153600\n154624\n155648\n156672\n157696\n158720\n159744\n160768\n161792\n162816\n163840\n164864\n165888\n166912\n167936\n168960\n169984\n171008\n172032\n173056\n174080\n175104\n176128\n177152\n178176\n179200\n180224\n181248\n182272\n183296\n184320\n185344\n186368\n187392\n188416\n189440\n190464\n191488\n192512\n193536\n194560\n195584\n196608\n197632\n198656\n199680\n200704\n201728\n202752\n203776\n204800\n205824\n206848\n207872\n208896\n209920\n210944\n211968\n212992\n214016\n215040\n216064\n217088\n218112\n219136\n220160\n221184\n222208\n223232\n224256\n225280\n226304\n227328\n228352\n229376\n230400\n231424\n232448\n233472\n234496\n235520\n236544\n237568\n238592\n239616\n240640\n241664\n242688\n243712\n244736\n245760\n246784\n247808\n248832\n249856\n250880\n251904\n252928\n253952\n254976\n256000\n257024\n258048\n259072\n260096\n261120\n262144\n263168\n264192\n265216\n266240\n267264\n268288\n269312\n270336\n271360\n272384\n273408\n274432\n275456\n276480\n277504\n278528\n279552\n280576\n281600\n282624\n283648\n284672\n285696\n286720\n287744\n288768\n289792\n290816\n291840\n292864\n293888\n294912\n295936\n296960\n297984\n299008\n300032\n301056\n302080\n303104\n304128\n305152\n306176\n307200\n308224\n309248\n310272\n311296\n312320\n313344\n314368\n315392\n316416\n317440\n318464\n319488\n320512\n321536\n322560\n323584\n324608\n325632\n326656\n327680\n328704\n329728\n330752\n331776\n332800\n333824\n334848\n335872\n336896\n337920\n338944\n339968\n340992\n342016\n343040\n344064\n345088\n346112\n347136\n348160\n349184\n350208\n351232\n352256\n353280\n354304\n355328\n356352\n357376\n358400\n359424\n360448\n361472\n362496\n363520\n364544\n365568\n366592\n367616\n368640\n369664\n370688\n371712\n372736\n373760\n374784\n375808\n376832\n377856\n378880\n379904\n380928\n381952\n382976\n384000\n385024\n386048\n387072\n388096\n389120\n390144\n391168\n392192\n393216\n394240\n395264\n396288\n397312\n398336\n399360\n400384\n401408\n402432\n403456\n404480\n405504\n406528\n407552\n408576\n409600\n410624\n411648\n412672\n413696\n414720\n415744\n416768\n417792\n418816\n419840\n420864\n421888\n422912\n423936\n424960\n425984\n427008\n428032\n429056\n430080\n431104\n432128\n433152\n434176\n435200\n436224\n437248\n438272\n439296\n440320\n441344\n442368\n443392\n444416\n445440\n446464\n447488\n448512\n449536\n450560\n451584\n452608\n453632\n454656\n455680\n456704\n457728\n458752\n459776\n460800\n461824\n462848\n463872\n464896\n465920\n466944\n467968\n468992\n470016\n471040\n472064\n473088\n474112\n475136\n476160\n477184\n478208\n479232\n480256\n481280\n482304\n483328\n484352\n485376\n486400\n487424\n488448\n489472\n490496\n491520\n492544\n493568\n494592\n495616\n496640\n497664\n498688\n499712\n500736\n501760\n502784\n503808\n504832\n505856\n506880\n507904\n508928\n509952\n510976\n512000\n513024\n514048\n515072\n516096\n517120\n518144\n519168\n520192\n521216\n522240\n523264\n524288\n525312\n526336\n527360\n528384\n529408\n530432\n531456\n532480\n533504\n534528\n535552\n536576\n537600\n538624\n539648\n540672\n541696\n542720\n543744\n544768\n545792\n546816\n547840\n548864\n549888\n550912\n551936\n552960\n553984\n555008\n556032\n557056\n558080\n559104\n560128\n561152\n562176\n563200\n564224\n565248\n566272\n567296\n568320\n569344\n570368\n571392\n572416\n573440\n574464\n575488\n576512\n577536\n578560\n579584\n580608\n581632\n582656\n583680\n584704\n585728\n586752\n587776\nPrepare batches val\nPB entered:  7634677760\ntrain_y created:  7634677760\ntrain_data deleted:  7634677760\ntrain_data deleted:  7643873280\ntrain_data deleted:  7656038400\ntrain_data deleted:  7667933184\ntrain_data deleted:  7680073728\ntrain_data deleted:  7692132352\n1252896\n147040\n1024\n1320856\n1320856\n1320856\ndata created:  7706456064\n[]\ntrain_y deleted:  7706456064\n0\n1024\n2048\n3072\n4096\n5120\n6144\n7168\n8192\n9216\n10240\n11264\n12288\n13312\n14336\n15360\n16384\n17408\n18432\n19456\n20480\n21504\n22528\n23552\n24576\n25600\n26624\n27648\n28672\n29696\n30720\n31744\n32768\n33792\n34816\n35840\n36864\n37888\n38912\n39936\n40960\n41984\n43008\n44032\n45056\n46080\n47104\n48128\n49152\n50176\n51200\n52224\n53248\n54272\n55296\n56320\n57344\n58368\n59392\n60416\n61440\n62464\n63488\n64512\n65536\n66560\n67584\n68608\n69632\n70656\n71680\n72704\n73728\n74752\n75776\n76800\n77824\n78848\n79872\n80896\n81920\n82944\n83968\n84992\n86016\n87040\n88064\n89088\n90112\n91136\n92160\n93184\n94208\n95232\n96256\n97280\n98304\n99328\n100352\n101376\n102400\n103424\n104448\n105472\n106496\n107520\n108544\n109568\n110592\n111616\n112640\n113664\n114688\n115712\n116736\n117760\n118784\n119808\n120832\n121856\n122880\n123904\n124928\n125952\n126976\n128000\n129024\n130048\n131072\n132096\n133120\n134144\n135168\n136192\n137216\n138240\n139264\n140288\n141312\n142336\n143360\n144384\n145408\n146432\n0\n","name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-a0c372589098>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     train(train_data, test_data, pro_num, corr_data, timestamp, args.timespan, model, optimizer, logger, saver, args.num_epochs,\n\u001b[0;32m--> 381\u001b[0;31m           args.batch_size, args.grad_clip)\n\u001b[0m\u001b[1;32m    382\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;31m#except RuntimeError:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-a0c372589098>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data, val_data, pro_num, corr_data, timestamp, timespan, model, optimizer, logger, saver, num_epochs, batch_size, grad_clip)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;31m# rel = compute_corr(item_inputs, item_ids, corr_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mrel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorr_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_ids\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_inputs\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0mitem_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputeRePos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimespan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \"\"\"\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Dispatch to specialized methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINT_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m_validate_indices\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpack_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misintlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m_unpack_index\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;34m'except boolean indexing where matrix and index '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             'are equal shapes.')\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0mbool_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compatible_boolean_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0mbool_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compatible_boolean_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbool_row\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m_compatible_boolean_index\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;31m# Presence of attribute `ndim` indicates a compatible array type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_first_element_bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_maybe_bool_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m_maybe_bool_ndarray\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \"\"\"Returns a compatible array if elements are boolean.\n\u001b[1;32m    333\u001b[0m     \"\"\"\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \"\"\"\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_partition(data):\n    usernum = 0\n    itemnum = 0\n    User = defaultdict(list)\n    user_train = {}\n    user_valid = {}\n    user_test = {}\n    # assume user/item index starting from 1\n    for index, row in data.iterrows():\n        #print(row)\n        u, i, order = row['user_id'], row['problem_id'], row['order_id']\n        if not(math.isnan(float(u)) or math.isnan(float(i)) or math.isnan(float(order))):\n            u = int(u)\n            i = int(i)\n            order = int(order)\n            usernum = max(u, usernum)\n            itemnum = max(i, itemnum)\n            User[u].append((i, order))\n\n    for user in User:\n        nfeedback = len(User[user])\n        if nfeedback < 3:\n            user_train[user] = User[user]\n            user_valid[user] = []\n            user_test[user] = []\n        else:\n            user_train[user] = User[user][:-2]\n            user_valid[user] = []\n            user_valid[user].append(User[user][-2])\n            user_test[user] = []\n            user_test[user].append(User[user][-1])\n    return [user_train, user_valid, user_test, usernum, itemnum]\n\ndata_assestments = pd.read_csv('../input/assestments-2009-2010/non_skill_builder_data_new.csv', header=0, error_bad_lines=False, warn_bad_lines=True)\n\npartition = data_partition(data_assestments)\nprint(partition[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}