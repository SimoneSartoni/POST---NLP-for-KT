{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Transformers and Natural Language Processing for Knowledge Tracing:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook will contain an implementation of applying TF-IDF to problems textual descriptions, after necessary data preprocessing and of using consine similarity on TF-IDF values to calculate similarity between problems and predict result for following problem. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import necessary libraries:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "#import needed libraries \n",
    "from urllib.request import urlretrieve\n",
    "import zipfile, os\n",
    "import time, sys, copy\n",
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import hunspell\n",
    "import psutil\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import smart_open\n",
    "import six\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from time import time  # To time our operations\n",
    "from gensim.models import KeyedVectors"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-22T15:40:34.750858Z",
     "iopub.execute_input": "2021-05-22T15:40:34.751217Z",
     "iopub.status.idle": "2021-05-22T15:40:35.357232Z",
     "shell.execute_reply.started": "2021-05-22T15:40:34.751179Z",
     "shell.execute_reply": "2021-05-22T15:40:35.356436Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-4111552dc951>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mcollections\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdefaultdict\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmath\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeature_extraction\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mTfidfVectorizer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcorpus\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mstopwords\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mword_tokenize\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'sklearn'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clone github repositories from previous works"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "#clone repositeries\n",
    "!git clone https://github.com/shalini1194/RKT\n",
    "#!git clone https://github.com/lyf-1/PEBG.git\n",
    "!git clone https://github.com/jhljx/GKT.git   \n",
    "!git clone https://github.com/Shivanandmn/Knowledge-Tracing-SAINT.git\n",
    "!git clone https://github.com/MaurizioFD/RecSys_Course_AT_PoliMi\n",
    "#copy repositories in working directory\n",
    "!cp -r ./RKT/* ./\n",
    "!cp -r ./RecSys_Course_AT_PoliMi/* ./\n",
    "\n",
    "!pip install hunspell\n",
    "\"\"\"!pip install -r ./RecSys_Course_AT_PoliMi/requirements.txt\n",
    "!python ./RecSys_Course_AT_PoliMi/run_compile_all_cython.py\n",
    "\"\"\""
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from Base.Similarity.Compute_Similarity import Compute_Similarity"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Assistments 2012/13"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from import_assistments_files import import_data as imp\n",
    "assistments_problems = imp(\"Assistments/problem_bodies/ASSISTments2012DataSet-ProblemBodies.csv\")\n",
    "print(assistments_problems[0])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-21T11:19:27.863934Z",
     "iopub.execute_input": "2021-05-21T11:19:27.864259Z",
     "iopub.status.idle": "2021-05-21T11:19:28.940224Z",
     "shell.execute_reply.started": "2021-05-21T11:19:27.864224Z",
     "shell.execute_reply": "2021-05-21T11:19:28.939291Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Knowledge_Tracing/data/Assistments/problem_bodies/ASSISTments2012DataSet-ProblemBodies.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-5-2735a668125b>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mimport_assistments_files\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mimport_data\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mimp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0massistments_problems\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimp\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Assistments/problem_bodies/ASSISTments2012DataSet-ProblemBodies.csv\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0massistments_problems\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Simone\\Universita\\5anno\\Thesis\\TransformersForKnowledgeTracing\\Knowledge_Tracing\\code\\import_assistments_files.py\u001B[0m in \u001B[0;36mimport_data\u001B[1;34m(file_name)\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[1;31m# Assistments 2012/13 dataset with problems textual descriptions\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[1;31m# Data folder + File name must be the path to dataset.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 9\u001B[1;33m     \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_folder\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfile_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlow_memory\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     10\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\simone sartoni\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    608\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    609\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 610\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    611\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    612\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\simone sartoni\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    460\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    461\u001B[0m     \u001B[1;31m# Create the parser.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 462\u001B[1;33m     \u001B[0mparser\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    463\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    464\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\simone sartoni\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    817\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    818\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 819\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    820\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    821\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\simone sartoni\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[1;34m(self, engine)\u001B[0m\n\u001B[0;32m   1048\u001B[0m             )\n\u001B[0;32m   1049\u001B[0m         \u001B[1;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1050\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore[call-arg]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1051\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1052\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\simone sartoni\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, src, **kwds)\u001B[0m\n\u001B[0;32m   1865\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1866\u001B[0m         \u001B[1;31m# open handles\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1867\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1868\u001B[0m         \u001B[1;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1869\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mkey\u001B[0m \u001B[1;32min\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;34m\"storage_options\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"encoding\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"memory_map\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"compression\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\simone sartoni\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_open_handles\u001B[1;34m(self, src, kwds)\u001B[0m\n\u001B[0;32m   1360\u001B[0m         \u001B[0mLet\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mreaders\u001B[0m \u001B[0mopen\u001B[0m \u001B[0mIOHanldes\u001B[0m \u001B[0mafter\u001B[0m \u001B[0mthey\u001B[0m \u001B[0mare\u001B[0m \u001B[0mdone\u001B[0m \u001B[1;32mwith\u001B[0m \u001B[0mtheir\u001B[0m \u001B[0mpotential\u001B[0m \u001B[0mraises\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1361\u001B[0m         \"\"\"\n\u001B[1;32m-> 1362\u001B[1;33m         self.handles = get_handle(\n\u001B[0m\u001B[0;32m   1363\u001B[0m             \u001B[0msrc\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1364\u001B[0m             \u001B[1;34m\"r\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\simone sartoni\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\common.py\u001B[0m in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    645\u001B[0m                 \u001B[0merrors\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"replace\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    646\u001B[0m             \u001B[1;31m# Encoding\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 647\u001B[1;33m             handle = open(\n\u001B[0m\u001B[0;32m    648\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    649\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './Knowledge_Tracing/data/Assistments/problem_bodies/ASSISTments2012DataSet-ProblemBodies.csv'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Junyi"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "input_folder = '../input/'\n",
    "# Junyi with problems textual descriptions \n",
    "# Data folder + File name must be the path to dataset. \n",
    "file_name = 'junyi-dataset/junyi_question_text.txt'\n",
    "df = pd.read_csv(os.path.join(input_folder, file_name),low_memory=False, sep = '#')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-21T11:19:35.784955Z",
     "iopub.execute_input": "2021-05-21T11:19:35.785323Z",
     "iopub.status.idle": "2021-05-21T11:19:35.814135Z",
     "shell.execute_reply.started": "2021-05-21T11:19:35.78529Z",
     "shell.execute_reply": "2021-05-21T11:19:35.813299Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Peking Online Judge (POJ)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "input_folder = '../input/'\n",
    "# POJ with problems textual descriptions \n",
    "# Data folder + File name must be the path to dataset. \n",
    "file_name = 'poj-dataset/poj_question_text.txt'\n",
    "df = pd.read_csv(os.path.join(input_folder, file_name),low_memory=False, sep = '\\n', names=[\"data\"])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-22T15:40:41.678608Z",
     "iopub.execute_input": "2021-05-22T15:40:41.67893Z",
     "iopub.status.idle": "2021-05-22T15:40:41.752623Z",
     "shell.execute_reply.started": "2021-05-22T15:40:41.678899Z",
     "shell.execute_reply": "2021-05-22T15:40:41.751818Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Texts preprocessing and cleaning:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "eng_dict = hunspell.HunSpell('/usr/share/hunspell/en_US.dic', '/usr/share/hunspell/en_US.aff')\n",
    "\n",
    "problem_ids, assistment_ids, bodies = df['problem_id'], df['assistment_id'], df['body']\n",
    "texts=[]\n",
    "texts_no_numbers=[]\n",
    "texts_only_existing_words=[]\n",
    "\n",
    "nltk.download('stopwords')\n",
    "for body in bodies:\n",
    "    words_set = set({})\n",
    "    words_set_no_numbers = set({})\n",
    "    words_set_only_existing_words = set({})\n",
    "    text = str(body).replace(' ', '#').replace('/', '#slash#').replace('<', '#lessthan#').replace('>', '#morethan#').replace(\",\", \"#comma#\").replace(\";\", \"#semicolon#\").replace(\".\", \"#point#\").replace(\"?\", \"#questionmark#\").replace(\"!\", \"exclamationpoint\").replace(\"=\", \"#equal#\").replace(\"\\\\\", \"#slash#\").replace(\"%\", \"#percentage#\").replace(\"\\\\t\", \"#\").replace(\"\\\\n\", \"#\").replace(\"\\t\", \"#\").replace(\"\\n\", \"#\").replace('\\\"', \"#quotationmark#\").replace(\"(\", \"#openroundbracket#\").replace(\")\", \"#closeroundbracket#\").replace(\"[\", \"#opensquarebracket#\").replace(\"]\", \"#closesquarebracket#\").replace(\"_\", \"#underscore#\").replace(\"&\", \"#ampersand#\").replace(\"}\", \"#closebrace#\").replace(\"{\", \"#openbrace#\").replace(\"+\", \"#plus#\").replace(\"-\",\"#minus#\").replace(\"*\", \"#multiplication#\").replace(\"€\",\"#euros#\").replace(\"$\",\"#dollar#\").replace(\"^\",\"#powerof#exponent#\")\n",
    "    text = str(text).split('#')\n",
    "    for i in range(0, len(text)):\n",
    "        text[i].lower()\n",
    "        if eng_dict.spell(text[i]):\n",
    "            words_set_only_existing_words.add(text[i])\n",
    "        \"\"\"if len(text[i])<20:\n",
    "            words_set.add(text[i])\n",
    "            if not (text[i].isdecimal()):\n",
    "                result = ''.join(el for el in text[i] if not el.isdigit())\n",
    "                words_set_no_numbers.add(result)\"\"\"\n",
    "    \"\"\"text = list(words_set)\n",
    "    text_no_numbers = list(words_set_no_numbers)\"\"\"\n",
    "    text_only_existing_words = list(words_set_only_existing_words)\n",
    "    for i in stopwords.words('english'):\n",
    "        i = i.lower()\n",
    "        \"\"\"if text.count(i)>0:\n",
    "            text.remove(i)\n",
    "        if text_no_numbers.count(i)>0:\n",
    "            text_no_numbers.remove(i)\"\"\"\n",
    "        if text_only_existing_words.count(i)>0:\n",
    "            text_only_existing_words.remove(i)\n",
    "            \n",
    "    \"\"\"texts.append(text)\n",
    "    texts_no_numbers.append(text_no_numbers)\"\"\"\n",
    "    texts_only_existing_words.append(text_only_existing_words)\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-20T13:00:46.610249Z",
     "iopub.execute_input": "2021-05-20T13:00:46.610623Z",
     "iopub.status.idle": "2021-05-20T13:02:35.015709Z",
     "shell.execute_reply.started": "2021-05-20T13:00:46.61059Z",
     "shell.execute_reply": "2021-05-20T13:02:35.014573Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "eng_dict = hunspell.HunSpell('/usr/share/hunspell/en_US.dic', '/usr/share/hunspell/en_US.aff')\n",
    "\n",
    "problem_names, questions, question_descriptions = df['question_name'], df['chinese_question'], df['chinese_question_desc']\n",
    "texts_only_existing_words=[]\n",
    "problem_ids = []\n",
    "nltk.download('stopwords')\n",
    "problem_ids = range(0, len(questions))\n",
    "for index in range(0, len(questions)):\n",
    "    words_set = set({})\n",
    "    text = str(questions[index]).replace(' ', '#').replace('/', '#slash#').replace('<', '#lessthan#').replace('>', '#morethan#').replace(\",\", \"#comma#\").replace(\";\", \"#semicolon#\").replace(\".\", \"#point#\").replace(\"?\", \"#questionmark#\").replace(\"!\", \"exclamationpoint\").replace(\"=\", \"#equal#\").replace(\"\\\\\", \"#slash#\").replace(\"%\", \"#percentage#\").replace(\"\\\\t\", \"#\").replace(\"\\\\n\", \"#\").replace(\"\\t\", \"#\").replace(\"\\n\", \"#\").replace('\\\"', \"#quotationmark#\").replace(\"(\", \"#openroundbracket#\").replace(\")\", \"#closeroundbracket#\").replace(\"[\", \"#opensquarebracket#\").replace(\"]\", \"#closesquarebracket#\").replace(\"_\", \"#underscore#\").replace(\"&\", \"#ampersand#\").replace(\"}\", \"#closebrace#\").replace(\"{\", \"#openbrace#\").replace(\"+\", \"#plus#\").replace(\"-\",\"#minus#\").replace(\"*\", \"#multiplication#\").replace(\"€\",\"#euros#\").replace(\"$\",\"#dollar#\").replace(\"^\",\"#powerof#exponent#\")\n",
    "    text = str(text).split('#')\n",
    "    text_desc = str(question_descriptions[index]).replace(' ', '#').replace('/', '#slash#').replace('<', '#lessthan#').replace('>', '#morethan#').replace(\",\", \"#comma#\").replace(\";\", \"#semicolon#\").replace(\".\", \"#point#\").replace(\"?\", \"#questionmark#\").replace(\"!\", \"exclamationpoint\").replace(\"=\", \"#equal#\").replace(\"\\\\\", \"#slash#\").replace(\"%\", \"#percentage#\").replace(\"\\\\t\", \"#\").replace(\"\\\\n\", \"#\").replace(\"\\t\", \"#\").replace(\"\\n\", \"#\").replace('\\\"', \"#quotationmark#\").replace(\"(\", \"#openroundbracket#\").replace(\")\", \"#closeroundbracket#\").replace(\"[\", \"#opensquarebracket#\").replace(\"]\", \"#closesquarebracket#\").replace(\"_\", \"#underscore#\").replace(\"&\", \"#ampersand#\").replace(\"}\", \"#closebrace#\").replace(\"{\", \"#openbrace#\").replace(\"+\", \"#plus#\").replace(\"-\",\"#minus#\").replace(\"*\", \"#multiplication#\").replace(\"€\",\"#euros#\").replace(\"$\",\"#dollar#\").replace(\"^\",\"#powerof#exponent#\")\n",
    "    text_desc = str(text_desc).split('#')\n",
    "    text = text\n",
    "    text = list(set(text) | set(text_desc))\n",
    "    for i in range(0, len(text)):\n",
    "        text[i].lower()\n",
    "        if eng_dict.spell(text[i]):\n",
    "            words_set.add(text[i])\n",
    "    text_only_existing_words = list(words_set)\n",
    "    for i in stopwords.words('english'):\n",
    "        i = i.lower()\n",
    "        if text_only_existing_words.count(i)>0:\n",
    "            text_only_existing_words.remove(i)\n",
    "    if text_only_existing_words.count('TIMEOUT')>0:\n",
    "        text_only_existing_words.remove('TIMEOUT')\n",
    "    if text_only_existing_words.count('ISSUE')>0:\n",
    "        text_only_existing_words.remove('ISSUE')\n",
    "    if text_only_existing_words.count('underscore')>0:\n",
    "        text_only_existing_words.remove('underscore')\n",
    "    texts_only_existing_words.append(text_only_existing_words)\n",
    "print(texts_only_existing_words)"
   ],
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "eng_dict = hunspell.HunSpell('/usr/share/hunspell/en_US.dic', '/usr/share/hunspell/en_US.aff')\n",
    "number_to_index =dict({})\n",
    "questions = []\n",
    "index = 0\n",
    "questions.append([])\n",
    "for row in df['data']:\n",
    "    if '#' in row:\n",
    "        array = row.split('#')\n",
    "        if array[0].isdigit():\n",
    "            questions.append([])\n",
    "            index = index +1\n",
    "            number = int(array[0])\n",
    "            number_to_index[number] = index\n",
    "            questions[index] = array[1]\n",
    "            \n",
    "    else:\n",
    "        new = str(questions[index])+str(row)\n",
    "        questions[index] = new\n",
    "#print(phrases)\n",
    "texts_only_existing_words=[]\n",
    "\n",
    "nltk.download('stopwords')\n",
    "for index in range(0, len(questions)):\n",
    "    words_set = set({})\n",
    "    text = str(questions[index]).replace(' ', '#').replace('/', '#slash#').replace('<', '#lessthan#').replace('>', '#morethan#').replace(\",\", \"#comma#\").replace(\";\", \"#semicolon#\").replace(\".\", \"#point#\").replace(\"?\", \"#questionmark#\").replace(\"!\", \"exclamationpoint\").replace(\"=\", \"#equal#\").replace(\"\\\\\", \"#slash#\").replace(\"%\", \"#percentage#\").replace(\"\\\\t\", \"#\").replace(\"\\\\n\", \"#\").replace(\"\\t\", \"#\").replace(\"\\n\", \"#\").replace('\\\"', \"#quotationmark#\").replace(\"(\", \"#openroundbracket#\").replace(\")\", \"#closeroundbracket#\").replace(\"[\", \"#opensquarebracket#\").replace(\"]\", \"#closesquarebracket#\").replace(\"_\", \"#underscore#\").replace(\"&\", \"#ampersand#\").replace(\"}\", \"#closebrace#\").replace(\"{\", \"#openbrace#\").replace(\"+\", \"#plus#\").replace(\"-\",\"#minus#\").replace(\"*\", \"#multiplication#\").replace(\"€\",\"#euros#\").replace(\"$\",\"#dollar#\").replace(\"^\",\"#powerof#exponent#\")\n",
    "    text = str(text).split('#')\n",
    "    for i in range(0, len(text)):\n",
    "        text[i].lower()\n",
    "        if eng_dict.spell(text[i]):\n",
    "            words_set.add(text[i])\n",
    "    text_only_existing_words = list(words_set)\n",
    "    for i in stopwords.words('english'):\n",
    "        i = i.lower()\n",
    "        if text_only_existing_words.count(i)>0:\n",
    "            text_only_existing_words.remove(i)\n",
    "    if text_only_existing_words.count('TIMEOUT')>0:\n",
    "        text_only_existing_words.remove('TIMEOUT')\n",
    "    if text_only_existing_words.count('ISSUE')>0:\n",
    "        text_only_existing_words.remove('ISSUE')\n",
    "    if text_only_existing_words.count('underscore')>0:\n",
    "        text_only_existing_words.remove('underscore')\n",
    "    texts_only_existing_words.append(text_only_existing_words)\n",
    "problem_ids = number_to_index.keys()\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-22T15:44:44.338086Z",
     "iopub.execute_input": "2021-05-22T15:44:44.338419Z",
     "iopub.status.idle": "2021-05-22T15:44:45.187987Z",
     "shell.execute_reply.started": "2021-05-22T15:44:44.338386Z",
     "shell.execute_reply": "2021-05-22T15:44:45.186899Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# WORD2VEC using Genism:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "w2v_model = Word2Vec(min_count=2,\n",
    "                     window=5,\n",
    "                     vector_size=300,\n",
    "                     workers=3,\n",
    "                     sg = 1)\n",
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(texts_only_existing_words, progress_per=100)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "print(w2v_model)\n",
    "\n",
    "t = time()\n",
    "\n",
    "w2v_model.train(texts_only_existing_words, total_examples=w2v_model.corpus_count, epochs=10, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "\n",
    "w2v_model.save(\"word2vec.model\")\n",
    "word_vectors = w2v_model.wv\n",
    "word_vectors.save(\"word2vec.wordvectors\")\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load word2vec model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Assistments 2012/13 load word2vec saved vectors (not anymore trainable) \n",
    "#wv = KeyedVectors.load_word2vec_format(\"word2vec.model\")\n",
    "\n",
    "# Assistments 2012/13 load word2vec saved model (trainable) \n",
    "w2v_model = Word2Vec.load(\"word2vec.model\")\n",
    "word_vectors = w2v_model.wv\n",
    "similarity_matrix = np.dot(word_vectors.vectors, word_vectors.vectors.T)\n",
    "print(word_vectors.vectors.shape)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# junyi:\n",
    "# problem_ids = problem_names\n",
    "\n",
    "problem_to_text = dict({})\n",
    "\n",
    "vocabulary = word_vectors.key_to_index\n",
    "k = 0\n",
    "print(len(problem_ids))\n",
    "for (problem_id, t) in list(zip(*(problem_ids, texts_only_existing_words))):\n",
    "    problem_words = []\n",
    "    for word in t:\n",
    "        if word in vocabulary:\n",
    "            problem_words.append(word)\n",
    "    problem_to_text[k] = problem_words\n",
    "    k+=1\n",
    "print(k)\n",
    "pro_words = []"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-22T15:44:56.969885Z",
     "iopub.execute_input": "2021-05-22T15:44:56.970202Z",
     "iopub.status.idle": "2021-05-22T15:44:56.995077Z",
     "shell.execute_reply.started": "2021-05-22T15:44:56.970173Z",
     "shell.execute_reply": "2021-05-22T15:44:56.994002Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TSNE visualization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\n%matplotlib inline\n \nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\ndef tsnescatterplot(model, word, list_names):\n    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n    its list of most similar words, and a list of words.\n    \"\"\"\n    arrays = np.empty((0, 300), dtype='f')\n    word_labels = [word]\n    color_list  = ['red']\n\n    # adds the vector of the query word\n    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n    \n    # gets list of most similar words\n    close_words = model.wv.most_similar([word])\n    \n    # adds the vector for each of the closest words to the array\n    for wrd_score in close_words:\n        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n        word_labels.append(wrd_score[0])\n        color_list.append('blue')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n    \n    # adds the vector for each of the words from list_names to the array\n    for wrd in list_names:\n        wrd_vector = model.wv.__getitem__([wrd])\n        word_labels.append(wrd)\n        color_list.append('green')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n        \n    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n    reduc = PCA(n_components=20).fit_transform(arrays)\n    \n    # Finds t-SNE coordinates for 2 dimensions\n    np.set_printoptions(suppress=True)\n    \n    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n    \n     # Sets everything up to plot\n    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n                       'y': [y for y in Y[:, 1]],\n                       'words': word_labels,\n                       'color': color_list})\n    \n    fig, _ = plt.subplots()\n    fig.set_size_inches(9, 9)\n    \n    # Basic plot\n    p1 = sns.regplot(data=df,\n                     x=\"x\",\n                     y=\"y\",\n                     fit_reg=False,\n                     marker=\"o\",\n                     scatter_kws={'s': 40,\n                                  'facecolors': df['color']\n                                 }\n                    )\n    \n    # Adds annotations one by one with a loop\n    for line in range(0, df.shape[0]):\n         p1.text(df[\"x\"][line],\n                 df['y'][line],\n                 '  ' + df[\"words\"][line].title(),\n                 horizontalalignment='left',\n                 verticalalignment='bottom', size='medium',\n                 color=df['color'][line],\n                 weight='normal'\n                ).set_size(15)\n\n    \n    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n            \n    plt.title('t-SNE visualization for {}'.format(word.title()))\n    \ntsnescatterplot(w2v_model, 'power', [i[0] for i in model.wv.most_similar(negative=[\"power\"])])",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Calculate TF-IDF using libraries from scikit:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def identity_tokenizer(text):\n    return text\ntfidf_vectorizer_existing_words_only = []\ntfidf_vectorizer_existing_words_only = TfidfVectorizer(\n    analyzer='word',\n    tokenizer=identity_tokenizer,\n    preprocessor=identity_tokenizer,\n    token_pattern=None,\n    use_idf = True)\ntfidf_vectorizer_vectors_existing_words_only = tfidf_vectorizer_existing_words_only.fit_transform(texts_only_existing_words)\ndf_tf_idf_existing_words_only = pd.DataFrame.sparse.from_spmatrix(tfidf_vectorizer_vectors_existing_words_only)\nprint(df_tf_idf_existing_words_only.shape)\n\n\"\"\"tfIdfVectorizer=TfidfVectorizer(use_idf=True, stop_words= 'english')\ntfIdf = tfIdfVectorizer.fit_transform(texts_no_split)\ndf_tf_idf = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\ndf_tf_idf = df_tf_idf.sort_values('TF-IDF', ascending=False)\n#df_tf_idf = df_tf_idf[df_tf_idf>0 and (not math.isnan(df_tf_idf))]\nprint (df_tf_idf.head(50))\"\"\"\n#Now we change the name of the datasets available:\nsparse_tf_idf = tfidf_vectorizer_vectors_existing_words_only\ndataframe_tf_idf = df_tf_idf_existing_words_only\n\nwords_unique = tfidf_vectorizer_existing_words_only.get_feature_names()\n#Save sparse matrix in current directory\ndata_folder = './'\n\nsps.save_npz(os.path.join(data_folder, 'pro_words_existing_words_only.npz'), sparse_tf_idf)\n#sps.save_npz(os.path.join(data_folder, 'pro_words_removed_digits.npz'), tfidf_vectorizer_vectors_no_numbers)\n\nwords_dict = dict({})\nfor i in range(0, len(words_unique)):\n    words_dict[str(i)] = words_unique[i]\nprint(sparse_tf_idf)\n\ndef write_txt(file, data):\n    with open(file, 'w') as f:\n        for dd in data:\n            f.write(str(dd)+'\\n')\n                    \nwrite_txt(os.path.join(data_folder, 'words_set.txt'), words_unique)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-21T11:23:39.453111Z",
     "iopub.execute_input": "2021-05-21T11:23:39.453463Z",
     "iopub.status.idle": "2021-05-21T11:23:39.507407Z",
     "shell.execute_reply.started": "2021-05-21T11:23:39.453431Z",
     "shell.execute_reply": "2021-05-21T11:23:39.506599Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "shape of pro_word matrix before escaping any special character: (179950, 82084)\n\nIn the end I considered that escaping a symbol is better than removing, so I escape all the symbols, including { } * + - _ too, infact now symbols and words are cleaner, despite number has remained the same:    (179950, 62198)\n\nEventually we can consider removing words with digits to reduce number to: (179950, 44642)   --> 18000 less\n\nA good alternative is to use Hunspell library to check if the word exist, removing not existing ones. In this cas we reduce number of words to: (179950, 27742)   --> 17000 less  --> very good!!\n\nIf we consider only existing words and remove digits too we obtain:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Available dataset with TF-IDF values:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "pro_num = dataframe_tf_idf.shape[0]\nwords_num = dataframe_tf_idf.shape[1]\n#print(words_unique) \ndataframe_tf_idf    #dense pandas dataframe\nsparse_tf_idf       #sparse matrix\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Calculate cosine similarity between questions from TF-IDF dataset",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "shrink = 10\ntopK = 100\nnormalize = True\nsimilarity = \"cosine\"\nsimilarity_matrix = Compute_Similarity(sparse_tf_idf.T, shrink=shrink, topK=topK, normalize=normalize, similarity = similarity).compute_similarity()\nprint(similarity_matrix[0])\n\nsps.save_npz(os.path.join(data_folder, 'TF_IDF_pro_pro.npz'), similarity_matrix)\n\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from KNN.ItemKNNCBFRecommender import ItemKNNCBFRecommender\ndef get_URM(path):\n    process = psutil.Process(os.getpid())\n    gc.enable()\n    data = pd.read_csv(path, low_memory=False, encoding=\"ISO-8859-1\", dtype={\"user_id\": int, \"problem_id\": int, \"correct\": float})\n    user_list = data.user_id.to_list()\n    problem_list = data.problem_id.to_list()\n    y_list = data.correct.to_list()\n    for n, i in enumerate(y_list):\n        if i == 0.0:\n            y_list[n] = -1.0\n    del data\n    print(process.memory_info().rss)\n    URM_all = sps.coo_matrix((y_list, (user_list, problem_list)))\n    URM_all = URM_all.tocsr()\n    del y_list\n    del user_list\n    del problem_list\n    return URM_all\ndata_path = '../input/skillbuilder-data-2009-2010/2012-2013-data-with-predictions-4-final.csv'\nURM = get_URM(data_path)\nprint(URM)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "similarity_matrix = sps.load_npz('../input/assesments-12-13-precessed-data/TF_IDF_pro_pro.npz')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-20T12:41:13.908645Z",
     "iopub.execute_input": "2021-05-20T12:41:13.90913Z",
     "iopub.status.idle": "2021-05-20T12:41:29.523506Z",
     "shell.execute_reply.started": "2021-05-20T12:41:13.909095Z",
     "shell.execute_reply": "2021-05-20T12:41:29.522521Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "sparse_cb_similarity_matrix = sps.load_npz('../input/assesments-12-13-precessed-data/pro_pro_.npz')\nsparse_cb_similarity_matrix2 = sps.load_npz('../input/assesments-12-13-precessed-data/pro_pro_existing_words_only.npz')\n\n#CBF.save_model(folder_path='./')\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# **Evaluation of TF-IDF + cosine similarity**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def _compute_problem_score_from_pro_pro_matrix(sparse_similarity_matrix, user_profile_array, correct, target_problem):\n        \"\"\"\n        \n        \"\"\"\n        \"\"\"print(target_problem)\n        print(sparse_similarity_matrix.tocsr().getrow(target_problem).todense()[0])\"\"\"\n        item_scores = sparse_similarity_matrix.tocsr()[user_profile_array, :].dot(sparse_similarity_matrix.tocsr().getrow(target_problem).transpose())\n        item_scores = item_scores.transpose().todense().dot(correct)\n        if item_scores == 0.0:\n            return -10.0\n        return item_scores\n\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-21T11:23:55.397916Z",
     "iopub.execute_input": "2021-05-21T11:23:55.398234Z",
     "iopub.status.idle": "2021-05-21T11:23:55.403748Z",
     "shell.execute_reply.started": "2021-05-21T11:23:55.398206Z",
     "shell.execute_reply": "2021-05-21T11:23:55.402798Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# WORD2VEC:\ndef _compute_problem_score_from_pro_pro_matrix(word_vectors, user_profile_array, correct, target_problem):\n        \"\"\"\n        \n        \"\"\"\n        item_scores = []\n        if target_problem in problem_to_text:\n            for problem_id in user_profile_array:\n                if problem_id in problem_to_text and len(problem_to_text[problem_id])>0:\n                    similarity = word_vectors.n_similarity( problem_to_text[problem_id], problem_to_text[target_problem] )\n                else:\n                    similarity = 0.0\n                item_scores.append(similarity)\n            item_scores = np.array(item_scores).transpose().dot(correct)\n            return item_scores\n        return -10.0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-22T15:46:42.481264Z",
     "iopub.execute_input": "2021-05-22T15:46:42.481734Z",
     "iopub.status.idle": "2021-05-22T15:46:42.488561Z",
     "shell.execute_reply.started": "2021-05-22T15:46:42.481678Z",
     "shell.execute_reply": "2021-05-22T15:46:42.487663Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "data = np.load('../input/assesments-12-13-precessed-data/2012-2013-data-with-predictions-4-final.csv.npz')\ny, problems, real_lens = data['y'], data['problem'], data['real_len']\npro_num = data['problem_num']\ncorrects = np.where(y==-1.0, -100.0, y)\ncorrects = np.where(corrects==0.0, -1.0, corrects)\ncorrects = np.where(corrects ==-100.0, 0.0, corrects)\n\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-20T13:15:06.129869Z",
     "iopub.execute_input": "2021-05-20T13:15:06.130246Z",
     "iopub.status.idle": "2021-05-20T13:15:06.366014Z",
     "shell.execute_reply.started": "2021-05-20T13:15:06.130204Z",
     "shell.execute_reply": "2021-05-20T13:15:06.361497Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "data = pd.read_csv('../input/junyi-dataset/junyi.csv', sep = '\\n', names =['data'])\ndata = data['data']\nprint(data)\nindex = range(0, len(data)//4)\nreal_len_index = [el*4 for el in index]\nreal_lens = [int(data[x]) for x in real_len_index]\nproblem_index = [el*4+1 for el in index]\nproblem_data = [data[x].split(',') for x in problem_index]\ncorrects_index = [el*4+2 for el in index]\ncorrects_data = [data[x].split(',') for x in corrects_index]\nlabels =[]\npredictions_w2v =[]\nproblems = []\ncorrects = []\nfor problem, correct, real_len in list(zip(*(problem_data, corrects_data, real_lens))):\n    correct2 = [float(x) for x in correct]\n    problem2 = [int(x) for x in problem]\n    problem = problem2\n    correct = np.where(correct2==-1.0, -100.0, correct2)\n    correct = np.where(correct==0.0, -1.0, correct)\n    correct = np.where(correct ==-100.0, 0.0, correct)\n    problems.append(problem)\n    corrects.append(correct)\npro_num = len(problem)\n\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-21T11:33:49.624308Z",
     "iopub.execute_input": "2021-05-21T11:33:49.624656Z",
     "iopub.status.idle": "2021-05-21T11:34:26.155107Z",
     "shell.execute_reply.started": "2021-05-21T11:33:49.624618Z",
     "shell.execute_reply": "2021-05-21T11:34:26.154281Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "data = pd.read_csv('../input/poj-dataset/poj_log.csv')\nusers = set(data['User'])\nprint(len(users))\nreal_lens = []\nproblems = []\ncorrects = []\nfor user, problem in data.groupby(\"User\"):\n    correct_answer = problem['Result']\n    correct = []\n    problem_df = problem[\"Problem\"]\n    problem_list = []\n    k = 0\n    for p, c in list(zip(*(problem_df, correct_answer))):\n        if p in number_to_index:\n            problem_list.append(number_to_index[p])\n            if c == \"Accepted\":\n                correct.append(1.0)\n            else:\n                correct.append(-1.0)\n            k+=1\n    if k>1:\n        real_lens.append(k)\n        problems.append(problem_list)\n        corrects.append(correct)\nprint(problems[1])\nprint(corrects[1])\nprint(real_lens[1])",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-22T15:47:27.16264Z",
     "iopub.execute_input": "2021-05-22T15:47:27.163017Z",
     "iopub.status.idle": "2021-05-22T15:47:32.864373Z",
     "shell.execute_reply.started": "2021-05-22T15:47:27.162984Z",
     "shell.execute_reply": "2021-05-22T15:47:32.862933Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "problems_set_with_results = set([])\nfor problem, correct, real_len in list(zip(*(problems, corrects, real_lens))):\n    # TF-IDF:\n    problems_set_with_results = problems_set_with_results.union(set(problem))\nproblems_set_with_text = set(problem_ids)\nprint(problems_set_with_text)\nprint(problems_set_with_results)\nproblems_set_text_and_results = problems_set_with_text.intersection(problems_set_with_results)\nprint(len(problems_set_text_and_results))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-22T15:51:40.254889Z",
     "iopub.execute_input": "2021-05-22T15:51:40.255218Z",
     "iopub.status.idle": "2021-05-22T15:51:40.462076Z",
     "shell.execute_reply.started": "2021-05-22T15:51:40.255185Z",
     "shell.execute_reply": "2021-05-22T15:51:40.461222Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "predictions_TF_IDF =[]\npredictions_w2v = []\nrandom_predictions =[]\nlabels = []\ni = 0\nmax_prob = 0\nprint(np.amax(problems))\nprint(len(problems))\nfor problem, correct, real_len in list(zip(*(problems, corrects, real_lens))):\n    # TF-IDF:\n    problem2 = []\n    correct2 = []\n    target_problem = problem[real_len-1]\n    for p in range(0, len(problem)):\n        if p in problems_set_text_and_results:\n            problem2.append(problem[p])\n            correct2.append(correct[p]) \n    real_len2 = len(problem2)\n    if target_problem in problems_set_text_and_results:\n        prediction_TF_IDF = _compute_problem_score_from_pro_pro_matrix(similarity_matrix, problem2[0:real_len2-1], correct2[0:real_len2-1], problem2[real_len2-1])  \n        # WORD2VEC:\n        #prediction_w2v = _compute_problem_score_from_pro_pro_matrix(word_vectors, problem[0:real_len-1], correct[0:real_len-1], problem[real_len-1]) \n        if prediction_TF_IDF >-10.0:\n            if prediction_TF_IDF >= 0:\n                predictions_TF_IDF.append(1)\n            else:\n                predictions_TF_IDF.append(0)\n            if correct2[real_len2-1] == 1.0:\n                labels.append(1)\n            else:\n                labels.append(0)\n            random_predictions.append(np.random.randint(low = 0, high=2))\n    i+=1\n    if i%1000==0:\n        print(i)\nprint(predictions_TF_IDF)\nprint(labels)\nprint(random_predictions)\nprint(max_prob)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-20T14:21:43.915131Z",
     "iopub.execute_input": "2021-05-20T14:21:43.915532Z",
     "iopub.status.idle": "2021-05-20T14:21:55.413291Z",
     "shell.execute_reply.started": "2021-05-20T14:21:43.915496Z",
     "shell.execute_reply": "2021-05-20T14:21:55.41196Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "problems_set2.save()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def write_txt(file, data):\n    with open(file, 'w') as file2:\n        for d in data:\n            file2.write(str(float(d)))\n            file2.write(' ')\n                    \nwrite_txt(os.path.join(data_folder, 'predictions.txt'), predictions)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "acc = accuracy_score(labels, predictions_TF_IDF)\nprint(acc)\nrandom_acc = accuracy_score(labels, random_predictions)\nprint(random_acc)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-20T14:13:46.323086Z",
     "iopub.execute_input": "2021-05-20T14:13:46.323491Z",
     "iopub.status.idle": "2021-05-20T14:13:46.340666Z",
     "shell.execute_reply.started": "2021-05-20T14:13:46.323425Z",
     "shell.execute_reply": "2021-05-20T14:13:46.339427Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}