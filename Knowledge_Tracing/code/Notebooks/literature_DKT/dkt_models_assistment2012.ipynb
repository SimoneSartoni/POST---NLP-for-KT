{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"EmpyFMkRReRU"},"outputs":[],"source":["\"\"\"!pip install mlflow\n","!pip install tensorflow\n","!pip install numpy==1.18.1\n","!pip install scikit-learn==0.22.1\n","!pip install hdbscan --no-cache-dir --no-binary :all: --no-build-isolation\n","!pip install bertopic\n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mdAraGjLUiFX"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TjV0LOmESkmk"},"outputs":[],"source":["!rm -rf /content/TransformersForKnowledgeTracing\n","!rm -rf /content/KnowledgeTracing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IoNhQC_cSUvS"},"outputs":[],"source":["!git clone \"https://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@github.com/SimoneSartoni/TransformersForKnowledgeTracing.git\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FskNj87rReR9"},"outputs":[],"source":["import os, sys     \n","!cp -r /content/TransformersForKnowledgeTracing/* ./   \n","package_paths = [\n","    '/Knowledge_Tracing',\n","]\n","\n","for pth in package_paths:\n","    sys.path.append(pth)\n","print(sys.path)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"5LnH0BX_ReU8"},"outputs":[],"source":["import gc\n","import tensorflow as tf\n","import numpy as np\n","# import hunspell\n","from sklearn.model_selection import train_test_split\n","from Knowledge_Tracing.code.models.encoding_models.count_vectorizer import count_vectorizer\n","from Knowledge_Tracing.code.data_processing.preprocess.process_data_assistments_2012 import process_data_assistments_2012\n","from Knowledge_Tracing.code.data_processing.preprocess.process_data_assistments_2009 import process_data_assistments_2009\n","\n","MASK_VALUE = -1.0  # The masking value cannot be zero.\n","\n","batch_size=32\n","shuffle=True\n","dataset_name='assistment_2012'\n","interactions_filepath=\"../input/assistmentds-2012/2012-2013-data-with-predictions-4-final.csv\"\n","output_filepath='/kaggle/working/'\n","texts_filepath='../input/assistment-texts/ASSISTments2012DataSet-ProblemBodies.csv'\n","min_df=2\n","max_df=1.0\n","min_questions=2\n","max_features=None\n","max_questions=200\n","n_rows=None\n","n_texts=None\n","personal_cleaning=False\n","words = set()\n","previous_words = set()\n","if dataset_name == 'assistment_2012':\n","    df, text_df = process_data_assistments_2012(min_questions=min_questions, max_questions=max_questions,\n","                                                interactions_filepath=interactions_filepath, output_filepath=output_filepath,\n","                                                texts_filepath=texts_filepath, n_rows=n_rows, n_texts=n_texts,\n","                                                make_sentences_flag=True, personal_cleaning=personal_cleaning)\n","elif dataset_name == 'assistment_2009':\n","    df, text_df = process_data_assistments_2009(min_questions=min_questions, max_questions=max_questions,\n","                                                interactions_filepath=interactions_filepath,\n","                                                texts_filepath=texts_filepath, output_filepath=output_filepath,\n","                                                n_rows=n_rows, n_texts=n_texts, make_sentences_flag=False, \n","                                                personal_cleaning=personal_cleaning)\n","\n","print(df)\n","\"\"\"df = df[['question_id', 'user_id', 'problem_id', 'correct']]\n","print(text_df)\n","min_df = 2\n","words = set()\n","previous_words = set()\n","for max_df in [1.0, 1e-2, 5e-3, 1e-3, 5e-4, 1e-4]:\n","    \n","    # Step 3.1 - Generate NLP extracted encoding for problems\n","    encode_model = count_vectorizer(min_df=min_df, max_df=max_df, binary=False, max_features=max_features)\n","    encode_model.fit(text_df, save_filepath)\n","    print(\"len:\")\n","    print(len(encode_model.count_vectorizer.get_feature_names()))\n","\n","    max_value = encode_model.words_num        \n","    words = set(encode_model.count_vectorizer.get_feature_names())\n","    if len(previous_words) > 0:\n","        removed_words = previous_words.difference(words)\n","        added_words = words.difference(previous_words)\n","    else:\n","        removed_words = ()\n","        added_words = words\n","    print(\"removed_words from previous to \", str(max_df))\n","    print(removed_words)\n","    print(\"added_words from previous to \", str(max_df))\n","    print(added_words)\n","    previous_words = words\n","words = set()\n","previous_words = set()\n","max_df = 1.0\n","\n","for min_df in [2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, 20, 32, 64, 128, 256, 0.01, 0.05, 0.1, 0.2]:\n","    \n","    # Step 3.1 - Generate NLP extracted encoding for problems\n","    encode_model = count_vectorizer(min_df=min_df, max_df=max_df, binary=False, max_features=max_features)\n","    encode_model.fit(text_df, save_filepath)\n","    print(\"len:\")\n","    print(len(encode_model.count_vectorizer.get_feature_names()))\n","    max_value = encode_model.words_num        \n","    words = set(encode_model.count_vectorizer.get_feature_names())\n","    if len(previous_words) > 0:\n","        removed_words = previous_words.difference(words)\n","        added_words = words.difference(previous_words)\n","    else:\n","        removed_words = ()\n","        added_words = words\n","    print(\"removed_words from previous to \", str(min_df))\n","    print(removed_words)\n","    print(\"added_words from previous to \", str(min_df))\n","    print(added_words)\n","    previous_words = words\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CLNnlb3lReVH"},"outputs":[],"source":["from Knowledge_Tracing.code.data_processing.load_preprocessed.load_preprocessed_data import load_preprocessed_texts, \\\n","    load_preprocessed_interactions\n","from ast import literal_eval\n","import pandas as pd\n","texts_filepath='../input/assistment-2012-processed/texts_processed.csv'\n","dtypes = {'problem_id': 'int64', 'body': \"string\", 'question_id': \"int64\"}\n","print(\"loading csv.....\")\n","texts_df = pd.read_csv(texts_filepath, dtype=dtypes)\n","texts_df['body'] = texts_df['body'].apply(lambda x: literal_eval(x))\n","print(texts_df['body'][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"bGJNjpO5ReVK"},"outputs":[],"source":["import tensorflow as tf\n","from Knowledge_Tracing.code.models.DKT_models.count_vect_DKT.version_1 import data_utils, count_vect_deepkt, metrics\n","\n","fn = \"../input/assistment-2012-preprocessed/interactions_processed.csv\"  # Dataset path\n","verbose = 1  # Verbose = {0,1,2}\n","best_model_weights = \"weights/bestmodel\"  # File to save the model.\n","\n","batch_size = 1024  # Batch size\n","\n","test_fraction = 0.2  # Portion of data to be used for testing\n","validation_fraction = 0.2  # Portion of training data to be used for validation\n","repository = \"../input/assistment-2012-preprocessed/texts_processed.csv\"\n","interactions_filepath = fn\n","results = {}\n","min_df = 2\n","train_set, val_set, test_set, nb_encodings = data_utils.load_dataset(batch_size=batch_size, shuffle=False,\n","                                                               dataset_name='assistment_2012',\n","                                                               interactions_filepath=interactions_filepath, \n","                                                               texts_filepath=repository,\n","                                                               min_df=min_df, max_df=5e-4, max_features=2000,\n","                                                               min_questions=2, max_questions=30,\n","                                                               n_rows=None, n_texts=None, personal_cleaning=True)\n","\n","print(nb_encodings)\n","\n","log_dir = \"logs\"  # Path to save the logs.\n","# optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)  # Optimizer to use\n","lstm_units = 200  # Number of LSTM units\n","epochs = 60  # Number of epochs to train\n","dropout_rate = 0.3  # Dropout rate\n","for lr in [1e-2, 1e-3, 1e-4]:\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)  # Optimizer to use\n","    student_model = count_vect_deepkt.clean_count_vect_DKTModel(\n","        nb_encodings=nb_encodings,\n","        hidden_units=lstm_units,\n","        dropout_rate=dropout_rate)\n","\n","    student_model.compile(\n","        optimizer=optimizer,\n","        metrics=[\n","            metrics.BinaryAccuracy(),\n","            metrics.AUC(),\n","            metrics.Precision(),\n","            metrics.Recall()\n","        ])\n","\n","    student_model.summary()\n","\n","    history = student_model.fit(dataset=train_set,\n","                                epochs=epochs,\n","                                verbose=verbose,\n","                                validation_data=val_set,\n","                                callbacks=[\n","                                    tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',\n","                                                                     patience=8, restore_best_weights=True),\n","                                    tf.keras.callbacks.CSVLogger(f\"{log_dir}/train.log\"),\n","                                    tf.keras.callbacks.ModelCheckpoint(best_model_weights,\n","                                                                       save_best_only=True,\n","                                                                       save_weights_only=True),\n","                                    tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n","                                ])\n","\n","    student_model.load_weights(best_model_weights)\n","\n","result = student_model.custom_evaluate(test_set, verbose=verbose)\n","print(result)\n","print(train_set)\n","print(val_set)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hMwZeuWtReVQ"},"outputs":[],"source":["import tensorflow as tf\n","from Knowledge_Tracing.code.models.DKT_models.count_vect_DKT.version_2 import data_utils, count_vect_deepkt, metrics\n","\n","\n","fn = \"../input/assistment-2012-preprocessed/interactions_processed.csv\"  # Dataset path\n","verbose = 1  # Verbose = {0,1,2}\n","best_model_weights = \"weights/bestmodel\"  # File to save the model.\n","\n","batch_size = 512  # Batch size\n","\n","test_fraction = 0.2  # Portion of data to be used for testing\n","validation_fraction = 0.2  # Portion of training data to be used for validation\n","repository = \"../input/assistment-2012-preprocessed/texts_processed.csv\"\n","interactions_filepath = fn\n","results = {}\n","min_df = 2\n","train_set, val_set, test_set, nb_encodings = data_utils.load_dataset(batch_size=batch_size, shuffle=False,\n","                                                               interactions_filepath=interactions_filepath, \n","                                                               texts_filepath=repository,\n","                                                               min_df=min_df, max_df=5e-4, max_features=2000,\n","                                                               min_seq_len=5, interaction_sequence_len=30)\n","\n","print(nb_encodings)\n","\n","log_dir = \"logs\"  # Path to save the logs.\n","optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)  # Optimizer to use\n","lstm_units = 100  # Number of LSTM units\n","epochs = 60  # Number of epochs to train\n","dropout_rate = 0.3  # Dropout rate\n","student_model = count_vect_deepkt.clean_count_vect_DKTModel(\n","    nb_encodings=nb_encodings,\n","    hidden_units=lstm_units,\n","    dropout_rate=dropout_rate)\n","\n","student_model.compile(\n","    optimizer=optimizer,\n","    metrics=[\n","        metrics.BinaryAccuracy(),\n","        metrics.AUC(),\n","        metrics.Precision(),\n","        metrics.Recall()\n","    ])\n","\n","student_model.summary()\n","\n","history = student_model.fit(dataset=train_set,\n","                            epochs=epochs,\n","                            verbose=verbose,\n","                            validation_data=val_set,\n","                            callbacks=[\n","                                tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',\n","                                                                 patience=8, restore_best_weights=True),\n","                                tf.keras.callbacks.CSVLogger(f\"{log_dir}/train.log\"),\n","                                tf.keras.callbacks.ModelCheckpoint(best_model_weights,\n","                                                                   save_best_only=True,\n","                                                                   save_weights_only=True),\n","                                tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n","                            ])\n","\n","student_model.load_weights(best_model_weights)\n","\n","result = student_model.custom_evaluate(test_set, verbose=verbose)\n","print(result)\n","print(train_set)\n","print(val_set)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VrQoaiGKReVY"},"outputs":[],"source":["import tensorflow as tf\n","from Knowledge_Tracing.code.models.DKT_models.count_vect_DKT.count_vect_DKT_doubled_encodings import data_utils, count_vect_deepkt, metrics\n","\n","fn = \"../input/assistment-2012-processed/interactions_processed.csv\"  # Dataset path\n","verbose = 1  # Verbose = {0,1,2}\n","best_model_weights = \"weights/bestmodel\"  # File to save the model.\n","\n","batch_size = 512  # Batch size\n","\n","test_fraction = 0.2  # Portion of data to be used for testing\n","validation_fraction = 0.2  # Portion of training data to be used for validation\n","repository = \"../input/assistment-2012-processed/texts_processed.csv\"\n","interactions_filepath = fn\n","results = {}\n","min_df = 2\n","\"\"\"dictionary = {'user_id': 'int32', 'problem_id': 'int64',\n","              'correct': 'float64', \n","              'skill': \"int32\",\n","              'timestamp': \"string\", 'question_id': \"int64\"}\"\"\"\n","dictionary = None\n","train_set, val_set, test_set, nb_encodings = data_utils.load_dataset(batch_size=batch_size, shuffle=False,\n","                                                               interactions_filepath=interactions_filepath, \n","                                                               texts_filepath=repository,\n","                                                               min_df=min_df, max_df=5e-4, max_features=2048,\n","                                                               interaction_sequence_len=30, min_seq_len=5, dictionary=dictionary)\n","\n","print(nb_encodings)\n","log_dir = \"logs\"  # Path to save the logs.\n","optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)  # Optimizer to use\n","lstm_units = 512  # Number of LSTM units\n","epochs = 20  # Number of epochs to train\n","dropout_rate = 0.3  # Dropout rate\n","for lr in [1e-2]:\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)  # Optimizer to use\n","    student_model = count_vect_deepkt.clean_count_vect_DKTModel(\n","        nb_encodings=nb_encodings,\n","        hidden_units=lstm_units,\n","        dropout_rate=dropout_rate)\n","\n","    student_model.compile(\n","        optimizer=optimizer,\n","        metrics=[\n","            metrics.BinaryAccuracy(),\n","            metrics.AUC(),\n","            metrics.Precision(),\n","            metrics.Recall()\n","        ])\n","\n","    student_model.summary()\n","\n","    history = student_model.fit(dataset=train_set,\n","                                epochs=epochs,\n","                                verbose=verbose,\n","                                validation_data=val_set,\n","                                callbacks=[\n","                                    tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',\n","                                                                     patience=3, restore_best_weights=True),\n","                                    tf.keras.callbacks.CSVLogger(f\"{log_dir}/train.log\"),\n","                                    tf.keras.callbacks.ModelCheckpoint(best_model_weights,\n","                                                                       save_best_only=True,\n","                                                                       save_weights_only=True),\n","                                    tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n","                                ])\n","\n","    student_model.load_weights(best_model_weights)\n","\n","    result = student_model.custom_evaluate(test_set, verbose=verbose)\n","    print(result)\n","    results[lr] = result\n","print(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mmf1xVpqReVf"},"outputs":[],"source":["result = student_model.custom_evaluate(test_set, verbose=verbose)\n","print(result)\n","results[lr] = result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xa8D7vfTReVh"},"outputs":[],"source":["predictions = student_model.predict(test_set, verbose=verbose)\n","print(predictions)\n","count = 0\n","more_than_half = 0\n","for prediction in predictions:\n","    for pred in prediction:\n","        if pred<0.5:\n","            more_than_half += 1\n","        count+=1\n","print(more_than_half)\n","print(count)\n","print(more_than_half/count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QuqmuXvIReVo"},"outputs":[],"source":["import tensorflow as tf\n","from Knowledge_Tracing.code.models.DKT_models.DKT.standard_on_skill import data_utils, deepkt, metrics\n","\n","fn = \"../input/assistment-2012-processed/interactions_processed.csv\"  # Dataset path\n","verbose = 1  # Verbose = {0,1,2}\n","best_model_weights = \"weights/bestmodel\"  # File to save the model.\n","batch_size = 512\n","\n","test_fraction = 0.2  # Portion of data to be used for testing\n","validation_fraction = 0.2  # Portion of training data to be used for validation\n","repository = \"../input/assistment-2012-processed/texts_processed.csv\"\n","interactions_filepath = fn\n","dictionary = {'user_id': 'int32', 'problem_id': 'int64',\n","              'correct': 'float64', \n","              'skill': \"int32\",\n","              'timestamp': \"string\", 'question_id': \"int64\"}\n","train_set, val_set, test_set, features_depth, nb_skills = data_utils.load_dataset(batch_size=batch_size, shuffle=False,\n","                                                               interactions_filepath=interactions_filepath, \n","                                                               texts_filepath=repository,\n","                                                               interaction_sequence_len=30, min_seq_len=5, dictionary=dictionary)\n","\n","\n","\"\"\"train_set, val_set, test_set, nb_features, nb_skills = data_utils.load_dataset(shuffle=True,\n","                                                               dataset_name='assistment_2009',\n","                                                               interactions_filepath=interactions_filepath, \n","                                                               texts_filepath=repository,\n","                                                               min_df=2, max_df=1.0, max_features=None,\n","                                                               min_questions=2, max_questions=30,\n","                                                               n_rows=None, n_texts=None, personal_cleaning=False)\n","\"\"\"\n","\n","batch_size = 512  # Batch size\n","\n","\n","\n","log_dir = \"logs\"  # Path to save the logs.\n","optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)  # Optimizer to use\n","lstm_units = 100  # Number of LSTM units\n","\n","epochs = 60  # Number of epochs to train\n","dropout_rate = 0.3  # Dropout rate\n","\n","student_model = deepkt.DKTModel(\n","    nb_features=features_depth, \n","    nb_skills = nb_skills,\n","    hidden_units=lstm_units,\n","    dropout_rate=dropout_rate)\n","\n","student_model.compile(\n","    optimizer=optimizer,\n","    metrics=[\n","        metrics.BinaryAccuracy(),\n","        metrics.AUC(),\n","        metrics.Precision(),\n","        metrics.Recall()\n","    ])\n","\n","student_model.summary()\n","\n","history = student_model.fit(dataset=train_set,\n","                            epochs=epochs,\n","                            verbose=verbose,\n","                            validation_data=val_set,\n","                            callbacks=[\n","                                tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',\n","                                                                 patience=30, restore_best_weights=True),\n","                                tf.keras.callbacks.CSVLogger(f\"{log_dir}/train.log\"),\n","                                tf.keras.callbacks.ModelCheckpoint(best_model_weights,\n","                                                                   save_best_only=True,\n","                                                                   save_weights_only=True),\n","                                tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n","                            ])\n","\n","student_model.load_weights(best_model_weights)\n","\n","result = student_model.custom_evaluate(test_set, verbose=verbose)\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"oFWrk6VIReWA"},"outputs":[],"source":["import tensorflow as tf\n","from Knowledge_Tracing.code.models.DKT_models.DKT.version_1 import data_utils, deepkt, metrics\n","\n","fn = \"../input/assistments-2009-processed/interactions_processed.csv\"  # Dataset path\n","verbose = 1  # Verbose = {0,1,2}\n","best_model_weights = \"weights/bestmodel\"  # File to save the model.\n","batch_size = 512\n","\n","test_fraction = 0.2  # Portion of data to be used for testing\n","validation_fraction = 0.2  # Portion of training data to be used for validation\n","repository = \"../input/assistments-2009-processed/texts_processed.csv\"\n","interactions_filepath = fn\n","dictionary = {'user_id': 'int32', 'problem_id': 'int64',\n","              'correct': 'float64', \n","              'skill': \"int32\",\n","              'timestamp': \"string\", 'question_id': \"int64\"}\n","train_set, val_set, test_set, features_depth = data_utils.load_dataset(batch_size=batch_size, shuffle=False,\n","                                                               interactions_filepath=interactions_filepath, \n","                                                               texts_filepath=repository,\n","                                                               interaction_sequence_len=30, min_seq_len=5, dictionary=dictionary)\n","\n","\n","\"\"\"train_set, val_set, test_set, nb_features, nb_skills = data_utils.load_dataset(shuffle=True,\n","                                                               dataset_name='assistment_2009',\n","                                                               interactions_filepath=interactions_filepath, \n","                                                               texts_filepath=repository,\n","                                                               min_df=2, max_df=1.0, max_features=None,\n","                                                               min_questions=2, max_questions=30,\n","                                                               n_rows=None, n_texts=None, personal_cleaning=False)\n","\"\"\"\n","\n","batch_size = 512  # Batch size\n","\n","\n","\n","log_dir = \"logs\"  # Path to save the logs.\n","optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)  # Optimizer to use\n","lstm_units = 100  # Number of LSTM units\n","\n","epochs = 100  # Number of epochs to train\n","dropout_rate = 0.3  # Dropout rate\n","\n","student_model = deepkt.DKTModel(\n","    nb_features=features_depth, \n","    hidden_units=lstm_units,\n","    dropout_rate=dropout_rate)\n","\n","student_model.compile(\n","    optimizer=optimizer,\n","    metrics=[\n","        metrics.BinaryAccuracy(),\n","        metrics.AUC(),\n","        metrics.Precision(),\n","        metrics.Recall()\n","    ])\n","\n","student_model.summary()\n","\n","history = student_model.fit(dataset=train_set,\n","                            epochs=epochs,\n","                            verbose=verbose,\n","                            validation_data=val_set,\n","                            callbacks=[\n","                                tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',\n","                                                                 patience=30, restore_best_weights=True),\n","                                tf.keras.callbacks.CSVLogger(f\"{log_dir}/train.log\"),\n","                                tf.keras.callbacks.ModelCheckpoint(best_model_weights,\n","                                                                   save_best_only=True,\n","                                                                   save_weights_only=True),\n","                                tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n","                            ])\n","\n","student_model.load_weights(best_model_weights)\n","\n","result = student_model.custom_evaluate(test_set, verbose=verbose)\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ORJ1JA-7ReWI"},"outputs":[],"source":["import tensorflow as tf\n","from Knowledge_Tracing.code.models.DKT_models.word2vec_DKT import word2vec_deepkt, training_data_utils, metrics\n","\n","fn = \"../input/assestments-2009-2010/skill_builder_data_corrected_collapsed.csv\"  # Dataset path\n","verbose = 1  # Verbose = {0,1,2}\n","best_model_weights = \"weights/bestmodel\"  # File to save the model.\n","\n","batch_size = 128  # Batch size\n","\n","test_fraction = 0.2  # Portion of data to be used for testing\n","validation_fraction = 0.2  # Portion of training data to be used for validation\n","repository = \"../input/assistments-texts/ASSISTments2012DataSet-ProblemBodies.csv\"\n","interactions_filepath = fn\n","results = {}\n","min_df = 2\n","keyedvectors = \"../input/pretrained-keyed-vectors/vectors.kv\"\n","train_set, val_set, test_set, nb_encodings = training_data_utils.load_dataset(batch_size=batch_size, shuffle=True,\n","                                                           dataset_name='assistment_2009',\n","                                                           interactions_filepath=interactions_filepath, \n","                                                           texts_filepath=repository,\n","                                                           min_df=min_df, max_df=0.2, max_features=None,\n","                                                           min_questions=2, max_questions=30,\n","                                                           n_rows=None, n_texts=None, personal_cleaning=True, \n","                                                           keyed_vectors=keyedvectors, normalize_encoding=False)\n","\n","print(nb_encodings)\n","log_dir = \"logs\"  # Path to save the logs.\n","optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)  # Optimizer to use\n","lstm_units = 100  # Number of LSTM units\n","epochs = 5  # Number of epochs to train\n","dropout_rate = 0.3  # Dropout rate\n","for lr in [1e-3, 1e-4, 1e-5]:\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)  # Optimizer to use\n","    student_model = word2vec_deepkt.word2vec_DKTModel(\n","        nb_encodings=nb_encodings,\n","        hidden_units=lstm_units,\n","        dropout_rate=dropout_rate)\n","\n","    student_model.compile(\n","        optimizer=optimizer,\n","        metrics=[\n","            metrics.BinaryAccuracy(),\n","            metrics.AUC(),\n","            metrics.Precision(),\n","            metrics.Recall()\n","        ])\n","\n","    student_model.summary()\n","\n","    history = student_model.fit(dataset=train_set,\n","                                epochs=epochs,\n","                                verbose=verbose,\n","                                validation_data=val_set,\n","                                callbacks=[\n","                                    tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',\n","                                                                     patience=5, restore_best_weights=True),\n","                                    tf.keras.callbacks.CSVLogger(f\"{log_dir}/train.log\"),\n","                                    tf.keras.callbacks.ModelCheckpoint(best_model_weights,\n","                                                                       save_best_only=True,\n","                                                                       save_weights_only=True),\n","                                    tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n","                                ])\n","\n","    student_model.load_weights(best_model_weights)\n","\n","    result = student_model.custom_evaluate(test_set, verbose=verbose)\n","    print(result)\n","    results[min_df] = result\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16fWsfDxReWw"},"outputs":[],"source":["from Knowledge_Tracing.code.models.sentence_transformers import data_utils, sentence_transformer_deepkt, metrics\n","from Knowledge_Tracing.code.models.DKT.data_utils import split_dataset\n","\n","fn = \"../input/assestments-2009-2010/skill_builder_data_corrected_collapsed.csv\"  # Dataset path\n","verbose = 1  # Verbose = {0,1,2}\n","best_model_weights = \"weights/bestmodel\"  # File to save the model.\n","\n","batch_size = 128  # Batch size\n","\n","test_fraction = 0.2  # Portion of data to be used for testing\n","validation_fraction = 0.2  # Portion of training data to be used for validation\n","repository = \"../input/assistments-texts/ASSISTments2012DataSet-ProblemBodies.csv\"\n","interactions_filepath = fn\n","\n","dataset, length, nb_encodings = data_utils.load_dataset(batch_size=batch_size, shuffle=False,\n","                                                        dataset_name='assistment_2009',\n","                                                        interactions_filepath=interactions_filepath, \n","                                                        texts_filepath=repository,\n","                                                        min_df=2, max_df=1.0, max_features=1000,\n","                                                        min_questions=2, max_questions=30,\n","                                                        n_rows=None, n_texts=None, personal_cleaning=False)\n","\n","print(dataset)\n","print(length)\n","print(nb_encodings)\n","train_set, test_set, val_set = split_dataset(dataset=dataset, total_size=length, test_fraction=test_fraction, val_fraction=validation_fraction)\n","print(train_set)\n","set_sz = length * batch_size\n","test_set_sz = (set_sz * test_fraction)\n","val_set_sz = (set_sz - test_set_sz) * validation_fraction\n","train_set_sz = set_sz - test_set_sz - val_set_sz\n","print(\"============= Data Summary =============\")\n","print(\"Total number of students: %d\" % set_sz)\n","print(\"Training set size: %d\" % train_set_sz)\n","print(\"Validation set size: %d\" % val_set_sz)\n","print(\"Testing set size: %d\" % test_set_sz)\n","print(\"========================================\")\n","\n","import tensorflow as tf\n","log_dir = \"logs\"  # Path to save the logs.\n","optimizer = \"adam\"  # Optimizer to use\n","lstm_units = 100  # Number of LSTM units\n","epochs = 30  # Number of epochs to train\n","dropout_rate = 0.3  # Dropout rate\n","\n","student_model = sentence_transformer_deepkt.sentence_transformer_deepkt(\n","    nb_encodings=nb_encodings,\n","    hidden_units=lstm_units,\n","    dropout_rate=dropout_rate)\n","\n","student_model.compile(\n","    optimizer=optimizer,\n","    metrics=[\n","        metrics.BinaryAccuracy(),\n","        metrics.AUC(),\n","        metrics.Precision(),\n","        metrics.Recall()\n","    ])\n","\n","student_model.summary()\n","\n","history = student_model.fit(dataset=train_set,\n","                            epochs=epochs,\n","                            verbose=verbose,\n","                            validation_data=val_set,\n","                            callbacks=[\n","                                tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',\n","                                                                 patience=20, restore_best_weights=True),\n","                                tf.keras.callbacks.CSVLogger(f\"{log_dir}/train.log\"),\n","                                tf.keras.callbacks.ModelCheckpoint(best_model_weights,\n","                                                                   save_best_only=True,\n","                                                                   save_weights_only=True),\n","                                tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n","                            ])\n","\n","student_model.load_weights(best_model_weights)\n","\n","result = student_model.custom_evaluate(test_set, verbose=verbose)\n","print(result)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k047kIsRReW_"},"outputs":[],"source":["import tensorflow as tf\n","from Knowledge_Tracing.code.models.DKT_models.count_vect_plus_skills_DKT import data_utils, count_vect_plus_skill_deepkt, metrics\n","\n","fn = \"../input/assestments-2009-2010/skill_builder_data_corrected_collapsed.csv\"  # Dataset path\n","verbose = 1  # Verbose = {0,1,2}\n","best_model_weights = \"weights/bestmodel\"  # File to save the model.\n","\n","batch_size = 128  # Batch size\n","min_df = 2\n","test_fraction = 0.2  # Portion of data to be used for testing\n","validation_fraction = 0.2  # Portion of training data to be used for validation\n","repository = \"../input/assistments-texts/ASSISTments2012DataSet-ProblemBodies.csv\"\n","interactions_filepath = fn\n","results = {}\n","dictionary = {'user_id': 'int32', 'problem_id': 'int64',\n","              'correct': 'float64', \n","              'skill': \"int32\",\n","              'timestamp': \"string\", 'question_id': \"int64\"}\n","train_set, val_set, test_set, nb_encodings = data_utils.load_dataset(batch_size=batch_size, shuffle=False,\n","                                                               interactions_filepath=interactions_filepath, \n","                                                               texts_filepath=repository,\n","                                                               min_df=min_df, max_df=1.0, max_features=None,\n","                                                               interaction_sequence_len=30, min_seq_len=3, dictionary=dictionary)\n","\n","train_set, test_set, val_set, nb_encodings, nb_skills = data_utils.load_dataset(batch_size=batch_size, shuffle=True,\n","                                                               dataset_name='assistment_2009',\n","                                                               interactions_filepath=interactions_filepath, \n","                                                               texts_filepath=repository,\n","                                                               min_df=min_df, max_df=1.0, max_features=None,\n","                                                               min_questions=2, max_questions=30,\n","                                                               n_rows=None, n_texts=None, personal_cleaning=True)\n","\n","\n","log_dir = \"logs\"  # Path to save the logs.\n","optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)  # Optimizer to use\n","lstm_units = 100  # Number of LSTM units\n","epochs = 60  # Number of epochs to train\n","dropout_rate = 0.3  # Dropout rate\n","student_model = count_vect_plus_skill_deepkt.count_vect_plus_skill_DKTModel(\n","    nb_encodings=nb_encodings,\n","    nb_skills=nb_skills,\n","    hidden_units=lstm_units,\n","    dropout_rate=dropout_rate)\n","\n","student_model.compile(\n","    optimizer=optimizer,\n","    metrics=[\n","        metrics.BinaryAccuracy(),\n","        metrics.AUC(),\n","        metrics.Precision(),\n","        metrics.Recall()\n","    ])\n","\n","student_model.summary()\n","\n","history = student_model.fit(dataset=train_set,\n","                            epochs=epochs,\n","                            verbose=verbose,\n","                            validation_data=val_set,\n","                            callbacks=[\n","                                tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',\n","                                                                 patience=20, restore_best_weights=True),\n","                                tf.keras.callbacks.CSVLogger(f\"{log_dir}/train.log\"),\n","                                tf.keras.callbacks.ModelCheckpoint(best_model_weights,\n","                                                                   save_best_only=True,\n","                                                                   save_weights_only=True),\n","                                tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n","                            ])\n","\n","student_model.load_weights(best_model_weights)\n","\n","result = student_model.custom_evaluate(test_set, verbose=verbose)\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4YFKZp6JReXH"},"outputs":[],"source":["from Knowledge_Tracing.code.models.DKT_models.BERTTopic_DKT import data_utils, BERTopic_deepkt, metrics\n","\n","\n","\n","verbose = 1  # Verbose = {0,1,2}\n","best_model_weights = \"weights/bestmodel\"  # File to save the model.\n","\n","batch_size = 128  # Batch size\n","\n","test_fraction = 0.2  # Portion of data to be used for testing\n","validation_fraction = 0.2  # Portion of training data to be used for validation\n","texts_filepath = \"../input/assistment-2012-preprocessed/texts_processed.csv\"\n","interactions_filepath = \"../input/assistment-2012-preprocessed/interactions_processed.csv\"\n","batch_size = 128  # Batch size\n","max_features = 1000\n","min_df = 3\n","max_df = 2e-4\n","train_set, val_set, test_set, nb_encodings = data_utils.load_dataset(batch_size=batch_size, shuffle=True,\n","                                                               interactions_filepath=interactions_filepath, \n","                                                               texts_filepath=texts_filepath,\n","                                                               min_df=min_df, max_df=max_df, max_features=max_features)\n","\n","print(nb_encodings)\n","log_dir = \"logs\"  # Path to save the logs.\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-3)  # Optimizer to use\n","lstm_units = 100  # Number of LSTM units\n","epochs = 60  # Number of epochs to train\n","dropout_rate = 0.3  # Dropout rate\n","for lr in [1e-4]:\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)  # Optimizer to use\n","    student_model = count_vect_deepkt.clean_count_vect_DKTModel(\n","        nb_encodings=nb_encodings,\n","        hidden_units=lstm_units,\n","        dropout_rate=dropout_rate)\n","\n","    student_model.compile(\n","        optimizer=optimizer,\n","        metrics=[\n","            metrics.BinaryAccuracy(),\n","            metrics.AUC(),\n","            metrics.Precision(),\n","            metrics.Recall()\n","        ])\n","\n","    student_model.summary()\n","\n","    history = student_model.fit(dataset=train_set,\n","                                epochs=epochs,\n","                                verbose=verbose,\n","                                validation_data=val_set,\n","                                callbacks=[\n","                                    tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',\n","                                                                     patience=6, restore_best_weights=True),\n","                                    tf.keras.callbacks.CSVLogger(f\"{log_dir}/train.log\"),\n","                                    tf.keras.callbacks.ModelCheckpoint(best_model_weights,\n","                                                                       save_best_only=True,\n","                                                                       save_weights_only=True),\n","                                    tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n","                                ])\n","\n","    student_model.load_weights(best_model_weights)\n","\n","    result = student_model.custom_evaluate(test_set, verbose=verbose)\n","    print(result)\n","    results[min_df] = result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SvBua1GcReXR"},"outputs":[],"source":["!pip install sentence-transformers\n","\n","from Knowledge_Tracing.code.models.DKT_models.complete_DKT import data_utils, complete_deepkt, metrics\n","import sentence_transformers\n","fn = \"../input/assestments-2009-2010/skill_builder_data_corrected_collapsed.csv\"  # Dataset path\n","verbose = 1  # Verbose = {0,1,2}\n","best_model_weights = \"weights/bestmodel\"  # File to save the model.\n","\n","batch_size = 128  # Batch size\n","encodings_kwargs = {'use_skills':True, 'count_vect':True, 'min_df':10, 'max_df':0.2, 'max_features':3000,\n","                    'sentence_encoder':True, 'sentence_encoder_model':'gpt2', \n","                    'pretrained_word2vec':False, 'word2vec':False}\n","    \n","test_fraction = 0.2  # Portion of data to be used for testing\n","validation_fraction = 0.2  # Portion of training data to be used for validation\n","repository = \"../input/assistments-texts/ASSISTments2012DataSet-ProblemBodies.csv\"\n","interactions_filepath = fn\n","train_set, val_set, test_set, encoding_sizes, nb_features = data_utils.load_dataset(batch_size=batch_size, shuffle=False,\n","                                                        dataset_name='assistment_2009',\n","                                                        interactions_filepath=interactions_filepath, \n","                                                        texts_filepath=repository,\n","                                                        min_questions=2, max_questions=30,\n","                                                        n_rows=None, n_texts=5000, personal_cleaning=False, \n","                                                        **encodings_kwargs)\n","\n","\n","\n","import tensorflow as tf\n","log_dir = \"logs\"  # Path to save the logs.\n","optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)  # Optimizer to use\n","lstm_units = 100  # Number of LSTM units\n","epochs = 30  # Number of epochs to train\n","dropout_rate = 0.3  # Dropout rate\n","\n","student_model = complete_deepkt.complete_DKTModel(\n","    encoding_sizes=encoding_sizes,\n","    nb_features=nb_features,\n","    hidden_units=lstm_units,\n","    dropout_rate=dropout_rate)\n","\n","student_model.compile(\n","    optimizer=optimizer,\n","    metrics=[\n","        metrics.BinaryAccuracy(),\n","        metrics.AUC(),\n","        metrics.Precision(),\n","        metrics.Recall()\n","    ])\n","\n","student_model.summary()\n","\n","history = student_model.fit(dataset=train_set,\n","                            epochs=epochs,\n","                            verbose=verbose,\n","                            validation_data=val_set,\n","                            callbacks=[\n","                                tf.keras.callbacks.EarlyStopping(monitor='val_auc', mode='max',\n","                                                                 patience=5, restore_best_weights=True),\n","                                tf.keras.callbacks.CSVLogger(f\"{log_dir}/train.log\"),\n","                                tf.keras.callbacks.ModelCheckpoint(best_model_weights,\n","                                                                   save_best_only=True,\n","                                                                   save_weights_only=True),\n","                                tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n","                            ])\n","\n","student_model.load_weights(best_model_weights)\n","\n","result = student_model.custom_evaluate(test_set, verbose=verbose)\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JhBRLcq1ReXU"},"outputs":[],"source":["print(result)"]}],"metadata":{"colab":{"name":"dkt_models_assistment2012.ipynb","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}